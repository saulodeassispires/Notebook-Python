{"cells":[{"cell_type":"code","source":["import json\nimport pyspark.sql.functions as F\nimport re\nimport sys\nimport traceback\nimport pandas as pd\nfrom datetime import datetime, date, timedelta\nfrom delta.tables import DeltaTable\nfrom enum import Enum, unique\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.window import Window\nfrom types import TracebackType\nfrom typing import List, Type, TypedDict\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import row_number, desc\n\n\n\nclass Framework:\n    \n    @unique\n    class LoadType(str, Enum):\n        OVERWRITE_TABLE = \"OVERWRITE_TABLE\"\n        OVERWRITE_PARTITION = \"OVERWRITE_PARTITION\"\n        APPEND_ALL = \"APPEND_ALL\"  \n        APPEND_NEW = \"APPEND_NEW\"\n        UPSERT = \"UPSERT\"\n\n    @unique\n    class RawFileFormat(str, Enum):\n        PARQUET = \"PARQUET\"\n        DELTA = \"DELTA\"\n        ORC = \"ORC\"\n        CSV = \"CSV\"\n\n    @unique\n    class RunStatus(str, Enum):\n        SUCCEEDED = \"SUCCEEDED\"\n        FAILED = \"FAILED\"\n    \n    @unique\n    class SchemaEvolutionMode(str, Enum):\n        FAIL_ON_SCHEMA_MISMATCH = \"FAIL_ON_SCHEMA_MISMATCH\"\n        ADD_NEW_COLUMNS = \"ADD_NEW_COLUMNS\"\n        IGNORE_NEW_COLUMNS = \"IGNORE_NEW_COLUMNS\"\n        OVERWRITE_SCHEMA = \"OVERWRITE_SCHEMA\"\n\n\n    class ReturnObject(TypedDict):\n        status: str\n        target_object: str\n        num_records_read: int\n        num_records_loaded: int\n        num_records_errored_out: int\n        error_message: str\n        error_details: str\n\n    \n    def check_workspace(environment)-> str:\n        ## busca workspace id\n        id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n        if id == 'xxxxxxxxx':\n            return  'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n        elif id == 'yyyyyyyyy':\n            return 'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n        elif id == 'zzzzzzzz':\n            return 'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n        else:\n            ##raise ValueError(\"Este workspace não pertence a este escopo \")\n            return f'dbfs:/mnt/lakehouse/{environment}'\n    \n    LAKEHOUSE_LANDING_ROOT =  check_workspace('land')\n    LAKEHOUSE_BRONZE_ROOT  =  check_workspace('bronze')\n    LAKEHOUSE_SILVER_ROOT  =  check_workspace('silver')\n    LAKEHOUSE_GOLD_ROOT    =  check_workspace('gold')\n    \n    @classmethod\n    def _build_return_object(\n        cls,\n        status: RunStatus,\n        target_object: str,\n        num_records_read: int = 0,\n        num_records_loaded: int = 0,\n        error_message: str = \"\",\n        error_details: str = \"\",\n    ) -> ReturnObject:\n        \"\"\" Retorna objeto com detalhes da ultima excução/erro\n        \"\"\"\n        return {\n            \"status\": status,\n            \"target_object\": target_object,\n            \"num_records_read\": num_records_read,\n            \"num_records_loaded\": num_records_loaded,\n            \"num_records_errored_out\": num_records_read - num_records_loaded,\n            \"error_message\": error_message[:8000],\n            \"error_details\": error_details,\n        }\n                \n    @classmethod\n    def exit_with_object(cls, results: ReturnObject):\n        \"\"\" Retorno de objeto com o resultado da execução \n        \"\"\"\n        dbutils.notebook.exit(json.dumps(results))\n    \n    \n    @classmethod\n    def exit_with_last_exception(cls):\n        \"\"\"Busca o ultimo erro e retorno um objeto.\n        \"\"\"\n        exc_type, exc_value, _ = sys.exc_info()\n        results = cls._build_return_object(\n            status=cls.RunStatus.FAILED,\n            target_object=None,\n            error_message=f\"{exc_type.__name__}: {exc_value}\",\n            error_details=traceback.format_exc(),\n        )\n        cls.exit_with_object(results)\n        \n\n        \n    @classmethod\n    def read_landing_zone_dataframe(\n        cls,\n        file_format: RawFileFormat,\n        location: str,\n        delimiter: str = ';',\n        quote: str = \"\",\n    ) -> DataFrame:\n        \"\"\"Leitura de arquivo da camada Landing com opções de parametros como csv, parquet, avro \"\"\"\n\n        try:\n            df = (\n                spark.read\n                .format(file_format.lower())\n                .option(\"header\", True)\n                .option(\"escape\", \"\\\"\")\n                .option(\"mergeSchema\", True)\n                .option(\"delimiter\", delimiter)\n                .option(\"quote\", quote)\n                .load(location)\n            )\n\n            # transforma todas as colunas para string\n            if file_format != cls.RawFileFormat.CSV:\n                non_string_columns = [col for col, dtype in df.dtypes if dtype != \"string\"]\n                for column in non_string_columns:\n                    df = df.withColumn(column, F.col(column).cast(\"string\"))\n\n            return df\n\n        except:\n            cls.exit_with_last_exception()\n            \n    @classmethod\n    def generate_bronze_table_location(\n        cls,\n        table_name: str,\n    ) -> str:\n        \"\"\"Cria caminho para tabela bronze \n        \"\"\"\n        try:\n            # Verifica se os parametro tem algum campo nulo ou em branco\n            params_list = [schema_name, table_name]\n            if any(len(x) == 0 for x in params_list):\n                raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n            return f\"{cls.LAKEHOUSE_BRONZE_ROOT}/{table_name}/\"\n\n            \n        except:\n            cls.exit_with_last_exception()\n\n            \n            \n    @classmethod\n    def generate_silver_table_location(\n        cls,\n        schema_name: str,\n        table_name: str,\n    ) -> str:\n        \"\"\"Cria caminho para tabela bronze \n        \"\"\"\n        \n        try:\n            # Verifica se os parametro tem algum campo nulo ou em branco\n            params_list = [schema_name, table_name]\n            if any(len(x) == 0 for x in params_list):\n                raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n            return f\"{cls.LAKEHOUSE_SILVER_ROOT}/{table_name}/\"\n\n            \n        except:\n            cls.exit_with_last_exception()\n            \n    @classmethod\n    def generate_gold_table_location(\n        cls,\n        schema_name: str,\n        table_name: str,\n    ) -> str:\n        \"\"\"Cria caminho para tabela bronze \n        \"\"\"\n        \n        try:\n            # Verifica se os parametro tem algum campo nulo ou em branco\n            params_list = [schema_name, table_name]\n            if any(len(x) == 0 for x in params_list):\n                raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n            return f\"{cls.LAKEHOUSE_GOLD_ROOT}/{table_name}/\"\n\n            \n        except:\n            cls.exit_with_last_exception()\n\n    @classmethod\n    def write_delta_table(\n        cls,\n        df: DataFrame,\n        location: str,\n        schema_name: str,\n        table_name: str,\n        load_type: LoadType,\n        key_columns: List[str] = [],\n        partition_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n      \n        \"\"\"Escreve um DataFrame como Delta Table \n        \"\"\"\n        num_records_read = 0\n        num_records_loaded = 0\n      \n        try:\n            # Tabela deve existir para aplicar o merge\n            if load_type != cls.LoadType.APPEND_ALL and not DeltaTable.isDeltaTable(spark, location):\n                #print(\"Delta table ainda não existe. Altere load_type para APPEND_ALL e reexecute\")\n                load_type = cls.LoadType.APPEND_ALL\n\n            # Otimiza a escrita para evitar small files \n            spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n\n            # Determina o load type\n            if load_type == cls.LoadType.APPEND_ALL:\n                cls._write_table_using_append_all(\n                    df=df,\n                    location=location,\n                    partition_columns=partition_columns,\n                    schema_evolution_mode=schema_evolution_mode,\n                )\n            elif load_type == cls.LoadType.UPSERT:\n                if len(key_columns) == 0:\n                    raise ValueError(\"Nenhuma coluna foi especificada para o upsert\")\n                     \n                cls._write_table_using_upsert(\n                    df=df,\n                    location=location,\n                    key_columns=key_columns,\n                    schema_evolution_mode=schema_evolution_mode,\n                )\n            else:\n                raise NotImplementedError\n\n            # Cria Metadados e tabela\n            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name};\")\n            spark.sql(f\"CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} USING DELTA LOCATION '{location}';\")\n\n            #return cls._build_return_object(\n            #    status=cls.RunStatus.SUCCEEDED,\n            #    target_object=f\"{schema_name}.{table_name}\",\n            #    num_records_read=num_records_read,\n            #    num_records_loaded=num_records_loaded,\n            #)\n\n        except Exception as e:\n            return cls._build_return_object(\n                status=cls.RunStatus.FAILED,\n                target_object=f\"{schema_name}.{table_name}\",\n                num_records_read=num_records_read,\n                num_records_loaded=num_records_loaded,\n                error_message=str(e),\n                error_details=traceback.format_exc(),\n            )\n            \n    @classmethod\n    def _write_table_using_append_all(\n        cls,\n        df: DataFrame,\n        location: str,\n        partition_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n        \"\"\"Escreve o DataFrame utilizando APPEND_ALL.\n        \"\"\"\n        \n        df_writer = (\n            df.write\n            .format(\"delta\")\n            .mode(\"append\")\n        )\n\n        # Checa se havera partições\n        if len(partition_columns) > 0:\n            df_writer = df_writer.partitionBy(partition_columns)\n\n        # verifica schema evolution\n        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n            pass\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n            df_writer = df_writer.option(\"mergeSchema\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n            if DeltaTable.isDeltaTable(spark, location):\n                table_columns = DeltaTable.forPath(spark, location).columns\n                new_df_columns = [col for col in df.columns if col not in table_columns]\n                df = df.drop(*new_df_columns)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n            df_writer = df_writer.option(\"overwriteSchema\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        # Grava a Delta Table\n        df_writer.save(location)   \n    \n    @classmethod\n    def _write_table_using_upsert(\n        cls,\n        df: DataFrame,\n        location: str,\n        key_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n        \"\"\"Atualiza Delta table utilizando UPSERT.\n        \"\"\"\n        # Set schema_evolution_mode options\n        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n            pass\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n            original_auto_merge = spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\")\n            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n            if DeltaTable.isDeltaTable(spark, location):\n                table_columns = DeltaTable.forPath(spark, location).columns\n                new_df_columns = [col for col in df.columns if col not in table_columns]\n                df = df.drop(*new_df_columns)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n            raise ValueError(\"OVERWRITE_SCHEMA não é suportado no UPSERT load type\")\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        # Constroi a condição de merge\n        merge_condition_parts = [f\"source.`{col}` = target.`{col}`\" for col in key_columns]\n        merge_condition = \" AND \".join(merge_condition_parts)\n\n        # Escreve a delta table\n        delta_table = DeltaTable.forPath(spark, location)\n        (\n            delta_table.alias(\"target\")\n            .merge(df.alias(\"source\"), merge_condition)\n            .whenMatchedUpdateAll()\n            .whenNotMatchedInsertAll()\n            .execute()\n        )\n\n        # Reset spark.conf\n        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", original_auto_merge)\n     \n    \n    ####################################################\n    ## codigo importado funções já existentes Bemol   ##\n    ####################################################\n    \n    ### Lista arquivos disponiveis na landing\n    @classmethod        \n    def folder_read_path(\n        cls, \n        source_table, \n        source_path_landing,\n        years: List[str] = [],\n    ) -> list:\n      \n        try:\n            if years == []: \n              years = [file.name[:-1] for file in dbutils.fs.ls(source_path_landing)]\n            regex = '\\d+X*_\\d+X*_*\\d*.csv$'\n            file_list = []\n            for year in years:\n              #lista de arquivos nas pastas\n              filenames = dbutils.fs.ls(source_path_landing + '/' + year)\n              for file in filenames:\n                # obtenção do mês do arquivo\n                start, _ = re.search(regex, file.path).span()    #FALAR COM O SAULO DESSA BRUXARIA\n                month = file.path[start+4:start+6]\n                file_list.append(file.path)\n            return file_list \n        except :\n            cls.exit_with_last_exception()\n            \n    ### Copia os arquivos da coletado para o diretório da processado\n    @staticmethod\n    def transport_collected_precessed(\n        file_list:List[str]):\n      \n        try:\n            for file in file_list:\n                dbutils.fs.mv(file, file.replace('Coletado', 'Processado'), True)  \n        except : \n            print('Erro ao tentar mover os arquivos')\n          \n    ### Leitura da tabela Bronze já aplicando deduplicação \n    @staticmethod  \n    def read_bronze(\n        schema_bronze,\n        bronze_table, \n        partition_key, \n        orderBy_key, \n        start_datetime, \n        end_datetime,\n        from_to_field, \n    ) -> DataFrame:\n      \n        try:\n          \n            df = (spark.sql(f'''SELECT * FROM {schema_bronze}.{bronze_table}''')\n                  .where((col(f\"{from_to_field}\") >= start_datetime) & (col(f\"{from_to_field}\") <= end_datetime) ))\n            windowSpec  = Window.partitionBy(f\"{partition_key}\").orderBy(col(f\"{orderBy_key}\").desc())\n            df = df.withColumn(\"row_number\",row_number().over(windowSpec)).filter('row_number = 1').drop('row_number')#.dropDuplicates([f\"{partition_key}\"]) verificar se precisa\n            \n            return df\n          \n        except :\n            print('Erro ao tentar selecionar tabela bronze')\n\n    ### Retorna o dicionario de dados de uma tabela especifica \n    @classmethod  \n    def _data_dictionary(\n        df,\n        target_table, \n        location: str=' ' , \n        delimiter: str='|',\n    ) -> DataFrame:\n        \n        try:\n          \n            table_data_dictionary = pd.read_csv('/dbfs/mnt/bemoldigitalde/Trusted/DATA_DICTIONARY/'+target_table+'.csv', sep='|') #puxa o dicionario da tabela\n\n            return table_data_dictionary \n              \n        except :\n            print('Erro ao tentar ler dicionario de dados')  \n      \n    ### Transforma a moeda americana em br-pt\n    @staticmethod \n    def decimal_br(decimal):\n        if decimal == None:\n          return None\n        return decimal.replace('.','').replace(',','.')\n      \n      \n    ### Decodificação e tipagem das colunas\n    @classmethod \n    def decode_columns(\n        cls, \n        df, \n        target_table,\n    ) -> DataFrame:\n      \n      table_data_dictionary = cls._data_dictionary(target_table)\n      columns_to_drop = list(table_data_dictionary[(table_data_dictionary['TRUSTED_EXCLUIDO'] == True)]['TRANSIENT_COLUNA'])#pegando colunas pra dropar\n\n      new_column_types = table_data_dictionary[(table_data_dictionary['TRUSTED_TIPO'] != 'string') & (table_data_dictionary['TRUSTED_TIPO'].notnull())][['TRANSIENT_COLUNA',                                    'TRUSTED_TIPO']].set_index('TRANSIENT_COLUNA').to_dict()['TRUSTED_TIPO'] #pegando a nova tipagem das colunas utilizadas\n\n      decode_indexes = list(table_data_dictionary[table_data_dictionary['ARQUIVO_DECODIFICADOR'].notnull()].index.values.astype(int))\n\n      description_tables = table_data_dictionary.iloc[decode_indexes, :][['TRANSIENT_COLUNA', 'ARQUIVO_DECODIFICADOR']].set_index('TRANSIENT_COLUNA').to_dict()['ARQUIVO_DECODIFICADOR']\n\n      map_dictionaries = table_data_dictionary.iloc[decode_indexes, :][['TRANSIENT_COLUNA', 'MAPEAMENTO_DECODIFICACAO']].set_index('TRANSIENT_COLUNA').to_dict()['MAPEAMENTO_DECODIFICACAO']\n\n      trusted_columns_not_decode = list(table_data_dictionary[(table_data_dictionary['TRUSTED_COLUNA'].notnull())&(table_data_dictionary['ARQUIVO_DECODIFICADOR'].isnull())]['TRANSIENT_COLUNA'])\n\n      all_columns_to_decode = []\n      \n      decode_columns_map = map_dictionaries.keys()\n      all_columns_to_decode += decode_columns_map\n\n      for column in description_tables.keys():\n        code_description = pd.read_csv('/dbfs/mnt/bemoldigitalde/Trusted/Arquivos_decodificacao/'+description_tables[column]+'-DECODE.csv', sep='|', dtype={'code':'object', 'description':'object'})\n        code_description = code_description.fillna('').set_index('code').to_dict()['description']\n\n        dict_map = eval(map_dictionaries[column])\n\n        decode_columns = dict_map.keys()\n        concat_decode_column_name = '_'.join(decode_columns) + '_X'\n\n        if len(decode_columns) == 1:\n            df = df.withColumn(concat_decode_column_name, col(column))\n        else:\n            agg_expression = \", lit('_'), \".join([\"col('\"+str(i)+\"')\" for i in decode_columns])\n            agg_expression = \"(\" + agg_expression + \")\"\n\n            df = df.withColumn(concat_decode_column_name, concat(*eval(agg_expression)))\n\n        if '' in code_description.keys():\n          df = df.fillna('', subset=[concat_decode_column_name])\n\n        df = df.replace(to_replace=code_description, subset=[concat_decode_column_name])\n\n        for column in decode_columns:\n          if column in trusted_columns_not_decode:\n            list(decode_columns).remove(column)\n\n        #all_columns_to_decode += decode_columns\n\n      drop_all_columns_to_decode = list(dict.fromkeys(all_columns_to_decode))\n\n      df = df.drop(*drop_all_columns_to_decode)\n      # drop de colunas vazias, nao utilizadas\n      df = df.drop(*columns_to_drop)\n\n      # modificação dos tipos de dados das colunas do dataframe\n      for column_name in new_column_types.keys():\n        column_type = new_column_types[column_name]\n        if column_type == 'timestamp':\n          df = df.withColumn(column_name, to_date(unix_timestamp(column_name, \"yyyyMMdd\").cast(column_type)))\n        elif column_type[:7] == 'decimal':\n          if column_type[:10] == 'decimal_br': \n            column_type = column_type.replace('_br','')\n            decimal_br_udf = udf(decimal_br)\n            df = df.withColumn(column_name, decimal_br_udf(column_name))\n          int_part = int(column_type[8:-1].split(',')[0])\n          decimal_places = int(column_type[8:-1].split(',')[1])\n          df = df.withColumn(column_name, df[column_name].cast(DecimalType(int_part,decimal_places)))\n        else:\n          df = df.withColumn(column_name, df[column_name].cast(column_type))\n      return df  \n    \n    \n    # funcao pra renomear as colunas de acordo com o dicionario de dados\n    @classmethod \n    def rename_collumns(\n        cls, \n        df, \n    ) -> DataFrame:\n      \n        table_data_dictionary = cls._data_dictionary(target_table)\n        # obtenção dos novos nomes das colunas, dataframe em pandas com colunas pra decodificar e com colunas sem decodificação\n        new_columns_names_with_decode = table_data_dictionary[(table_data_dictionary['MAPEAMENTO_DECODIFICACAO'].notnull())][['TRANSIENT_COLUNA', 'TRUSTED_COLUNA', 'MAPEAMENTO_DECODIFICACAO']]\n\n        new_columns_names_without_decode = table_data_dictionary[(table_data_dictionary['MAPEAMENTO_DECODIFICACAO'].isnull()) & (table_data_dictionary['TRUSTED_COLUNA'].notnull())][['TRANSIENT_COLUNA', 'TRUSTED_COLUNA']].set_index('TRANSIENT_COLUNA').to_dict()['TRUSTED_COLUNA']\n\n        #cria um dicionario com novos nomes da coluna\n        new_column_names = new_columns_names_without_decode.copy()\n\n        for _, row in new_columns_names_with_decode.iterrows():\n            new_column_names[('_'.join(list(eval(row['MAPEAMENTO_DECODIFICACAO']).keys())) + '_X')] = row['TRUSTED_COLUNA']\n\n        # renomeação dos nomes das colunas do dataframe\n        for old_column_name in new_column_names.keys():\n          new_column_name = new_column_names[old_column_name]\n          df = df.withColumnRenamed(old_column_name,new_column_name)\n        # insere zeros a esquerda na coluna ID_CLIENTE\n        if 'ID_CLIENTE' in df.columns:\n          F = df.withColumn('ID_CLIENTE', lpad(df['ID_CLIENTE'], 10, '0'))    #### verificar este item com James e LInconln ... nao deveria retornar F ? \n        return df\n      \n    # funcao pra remover as colunas que não estão presente no dicionario de dados \n    @classmethod   \n    def columns_to_keep(\n        cls, \n        df, \n        target_table, \n        extras_columns=[]\n    ) -> list:\n      \n        table_data_dictionary = cls._data_dictionary(target_table)\n        columns_to_keep = list(table_data_dictionary[(table_data_dictionary['TRUSTED_COLUNA']).notna()]['TRUSTED_COLUNA']) + extras_columns\n\n        return columns_to_keep\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef3904df-2ecf-4d03-9d04-b96346e58221","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fdea41e6-2963-4f39-b53c-94e2ef9eed17","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Classes_1.0","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2657659663112028}},"nbformat":4,"nbformat_minor":0}
