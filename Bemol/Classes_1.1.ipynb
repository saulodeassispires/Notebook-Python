{"cells":[{"cell_type":"code","source":["import json\nimport pyspark.sql.functions as F\nimport re\nimport sys\nimport traceback\nimport pandas as pd\nfrom datetime import datetime, date, timedelta\nfrom delta.tables import DeltaTable\nfrom enum import Enum, unique\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.window import Window\nfrom types import TracebackType\nfrom typing import List, Type, TypedDict\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import row_number, desc\n\n\n\nclass Framework:\n    \n    @unique\n    class LoadType(str, Enum):\n        OVERWRITE_TABLE = \"OVERWRITE_TABLE\"\n        OVERWRITE_PARTITION = \"OVERWRITE_PARTITION\"\n        APPEND_ALL = \"APPEND_ALL\"  \n        APPEND_NEW = \"APPEND_NEW\"\n        UPSERT = \"UPSERT\"\n\n    @unique\n    class RawFileFormat(str, Enum):\n        PARQUET = \"PARQUET\"\n        DELTA = \"DELTA\"\n        ORC = \"ORC\"\n        CSV = \"CSV\"\n\n    @unique\n    class RunStatus(str, Enum):\n        SUCCEEDED = \"SUCCEEDED\"\n        FAILED = \"FAILED\"\n        \n    @unique\n    class SchemaEvolutionMode(str, Enum):\n        FAIL_ON_SCHEMA_MISMATCH = \"FAIL_ON_SCHEMA_MISMATCH\"\n        ADD_NEW_COLUMNS = \"ADD_NEW_COLUMNS\"\n        IGNORE_NEW_COLUMNS = \"IGNORE_NEW_COLUMNS\"\n        OVERWRITE_SCHEMA = \"OVERWRITE_SCHEMA\"\n\n\n    class ReturnObject(TypedDict):\n        status: str\n        target_object: str\n        num_records_read: int\n        num_records_loaded: int\n        num_records_errored_out: int\n        error_message: str\n        error_details: str\n\n    \n    def check_workspace(environment)-> str:\n        ## busca workspace id\n        id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n        if id == 'xxxxxxxxx':\n            return  'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n        elif id == 'yyyyyyyyy':\n            return 'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n        elif id == 'zzzzzzzz':\n            return 'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n        else:\n            ##raise ValueError(\"Este workspace não pertence a este escopo \")\n            return f'dbfs:/mnt/lakehouse/{environment}'\n    \n    LAKEHOUSE_LANDING_ROOT =  check_workspace('land')\n    LAKEHOUSE_BRONZE_ROOT  =  check_workspace('bronze')\n    LAKEHOUSE_SILVER_ROOT  =  check_workspace('silver')\n    LAKEHOUSE_GOLD_ROOT    =  check_workspace('gold')\n    \n    @classmethod\n    def _build_return_object(\n        cls,\n        status: RunStatus,\n        target_object: str,\n        num_records_read: int = 0,\n        num_records_loaded: int = 0,\n        error_message: str = \"\",\n        error_details: str = \"\",\n    ) -> ReturnObject:\n        \"\"\" Retorna objeto com detalhes da ultima excução/erro\n        \"\"\"\n        return {\n            \"status\": status,\n            \"target_object\": target_object,\n            \"num_records_read\": num_records_read,\n            \"num_records_loaded\": num_records_loaded,\n            \"num_records_errored_out\": num_records_read - num_records_loaded,\n            \"error_message\": error_message[:8000],\n            \"error_details\": error_details,\n        }\n                \n    @classmethod\n    def exit_with_object(cls, results: ReturnObject):\n        \"\"\" Retorno de objeto com o resultado da execução \n        \"\"\"\n        dbutils.notebook.exit(json.dumps(results))\n    \n    \n    @classmethod\n    def exit_with_last_exception(cls):\n        \"\"\"Busca o ultimo erro e retorno um objeto.\n        \"\"\"\n        exc_type, exc_value, _ = sys.exc_info()\n        results = cls._build_return_object(\n            status=cls.RunStatus.FAILED,\n            target_object=None,\n            error_message=f\"{exc_type.__name__}: {exc_value}\",\n            error_details=traceback.format_exc(),\n        )\n        cls.exit_with_object(results)\n  \n  \n        \n    @classmethod\n    def read_landing_zone_dataframe(\n        cls,\n        file_format: RawFileFormat,\n        location: str,\n        delimiter: str = ';',\n        quote: str = \"\",\n    ) -> DataFrame:\n        \"\"\"Leitura de arquivo da camada Landing com opções de parametros como csv, parquet, avro \"\"\"\n\n        ###try:\n        df = (\n            spark.read\n            .format(file_format.lower())\n            .option(\"header\", True)\n            .option(\"escape\", \"\\\"\")\n            .option(\"mergeSchema\", True)\n            .option(\"delimiter\", delimiter)\n            .option(\"quote\", quote)\n            .load(location)\n        )\n\n        # transforma todas as colunas para string\n        if file_format != cls.RawFileFormat.CSV:\n            non_string_columns = [col for col, dtype in df.dtypes if dtype != \"string\"]\n            for column in non_string_columns:\n                df = df.withColumn(column, F.col(column).cast(\"string\"))\n\n        return df\n\n        ###except:\n        ###    cls.exit_with_last_exception()\n            \n    @classmethod\n    def generate_bronze_table_location(\n        cls,\n        table_name: str,\n    ) -> str:\n        \"\"\"Cria caminho para tabela bronze \n        \"\"\"\n        ###try:\n            # Verifica se os parametro tem algum campo nulo ou em branco\n        params_list = [schema_name, table_name]\n        if any(len(x) == 0 for x in params_list):\n            raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n        return f\"{cls.LAKEHOUSE_BRONZE_ROOT}/{table_name}/\"\n\n            \n        ###except:\n        ###    cls.exit_with_last_exception()\n\n            \n            \n    @classmethod\n    def generate_silver_table_location(\n        cls,\n        schema_name: str,\n        table_name: str,\n    ) -> str:\n        \"\"\"Cria caminho para tabela bronze \n        \"\"\"\n        \n        ###try:\n            # Verifica se os parametro tem algum campo nulo ou em branco\n        params_list = [schema_name, table_name]\n        if any(len(x) == 0 for x in params_list):\n            raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n        return f\"{cls.LAKEHOUSE_SILVER_ROOT}/{table_name}/\"\n\n            \n        ###except:\n        ###    cls.exit_with_last_exception()\n        \n    @classmethod\n    def generate_gold_table_location(\n        cls,\n        schema_name: str,\n        table_name: str,\n    ) -> str:\n        \"\"\"Cria caminho para tabela bronze \n        \"\"\"\n        \n        try:\n            # Verifica se os parametro tem algum campo nulo ou em branco\n            params_list = [schema_name, table_name]\n            if any(len(x) == 0 for x in params_list):\n                raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n            return f\"{cls.LAKEHOUSE_GOLD_ROOT}/{table_name}/\"\n\n            \n        except:\n            cls.exit_with_last_exception()\n\n    @classmethod\n    def write_delta_table(\n        cls,\n        df: DataFrame,\n        location: str,\n        schema_name: str,\n        table_name: str,\n        load_type: LoadType,\n        key_columns: List[str] = [],\n        partition_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n      \n        \"\"\"Escreve um DataFrame como Delta Table \n        \"\"\"\n        num_records_read = 0\n        num_records_loaded = 0\n      \n        ###try:\n            # Tabela deve existir para aplicar o merge\n        if load_type != cls.LoadType.APPEND_ALL and not DeltaTable.isDeltaTable(spark, location):\n            #print(\"Delta table ainda não existe. Altere load_type para APPEND_ALL e reexecute\")\n            load_type = cls.LoadType.APPEND_ALL\n\n        # Otimiza a escrita para evitar small files \n        spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n\n        # Determina o load type\n        if load_type == cls.LoadType.APPEND_ALL:\n            cls._write_table_using_append_all(\n                df=df,\n                location=location,\n                partition_columns=partition_columns,\n                schema_evolution_mode=schema_evolution_mode,\n            )\n        elif load_type == cls.LoadType.UPSERT:\n            if len(key_columns) == 0:\n                raise ValueError(\"Nenhuma coluna foi especificada para o upsert\")\n\n            cls._write_table_using_upsert(\n                df=df,\n                location=location,\n                key_columns=key_columns,\n                schema_evolution_mode=schema_evolution_mode,\n            )            \n        elif load_type == cls.LoadType.OVERWRITE_TABLE:\n               ## if num_records_read == 0:\n               ##     raise ValueError(\"Attempted to overwrite a table with an empty dataset. Operation aborted.\")\n                cls._write_table_using_overwrite_table(\n                    df=df,\n                    location=location,\n                    partition_columns=partition_columns,\n                    schema_evolution_mode=schema_evolution_mode,\n                )          \n        else:\n            raise NotImplementedError\n\n        # Cria Metadados e tabela\n        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name};\")\n        spark.sql(f\"CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} USING DELTA LOCATION '{location}';\")\n\n            #return cls._build_return_object(\n            #    status=cls.RunStatus.SUCCEEDED,\n            #    target_object=f\"{schema_name}.{table_name}\",\n            #    num_records_read=num_records_read,\n            #    num_records_loaded=num_records_loaded,\n            #)\n\n        ###except Exception as e:\n        ###    return cls._build_return_object(\n        ###        status=cls.RunStatus.FAILED,\n        ###        target_object=f\"{schema_name}.{table_name}\",\n        ###        num_records_read=num_records_read,\n        ###        num_records_loaded=num_records_loaded,\n        ###        error_message=str(e),\n        ###        error_details=traceback.format_exc(),\n        ###    )\n          \n                \n    @classmethod\n    def _write_table_using_overwrite_table(\n        cls,\n        df: DataFrame,\n        location: str,\n        partition_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n        \"\"\" Sobrepoe dados de table já existente \n        \"\"\"\n        df_writer = (\n            df.write\n            .format(\"delta\")\n            .mode(\"overwrite\")\n        )\n\n        # Set partition options\n        if len(partition_columns) > 0:\n            df_writer = df_writer.partitionBy(partition_columns)\n\n        # configura schema evolution mode \n        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n            pass\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n            df_writer = df_writer.option(\"mergeSchema\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n            if DeltaTable.isDeltaTable(spark, location):\n                table_columns = DeltaTable.forPath(spark, location).columns\n                new_df_columns = [col for col in df.columns if col not in table_columns]\n                df = df.drop(*new_df_columns)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n            df_writer = df_writer.option(\"overwriteSchema\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        # Write to the delta table\n        df_writer.save(location)\n                    \n          \n    @classmethod\n    def _write_table_using_append_all(\n        cls,\n        df: DataFrame,\n        location: str,\n        partition_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n        \"\"\"Escreve o DataFrame utilizando APPEND_ALL.\n        \"\"\"\n        \n        df_writer = (\n            df.write\n            .format(\"delta\")\n            .mode(\"append\")\n        )\n\n        # Checa se havera partições\n        if len(partition_columns) > 0:\n            df_writer = df_writer.partitionBy(partition_columns)\n\n        # verifica schema evolution\n        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n            pass\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n            df_writer = df_writer.option(\"mergeSchema\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n            if DeltaTable.isDeltaTable(spark, location):\n                table_columns = DeltaTable.forPath(spark, location).columns\n                new_df_columns = [col for col in df.columns if col not in table_columns]\n                df = df.drop(*new_df_columns)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n            df_writer = df_writer.option(\"overwriteSchema\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        # Grava a Delta Table\n        df_writer.save(location)   \n    \n    @classmethod\n    def _write_table_using_upsert(\n        cls,\n        df: DataFrame,\n        location: str,\n        key_columns: List[str] = [],\n        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n    ) -> ReturnObject:\n        \"\"\"Atualiza Delta table utilizando UPSERT.\n        \"\"\"\n        # Set schema_evolution_mode options\n        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n            pass\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n            original_auto_merge = spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\")\n            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n            if DeltaTable.isDeltaTable(spark, location):\n                table_columns = DeltaTable.forPath(spark, location).columns\n                new_df_columns = [col for col in df.columns if col not in table_columns]\n                df = df.drop(*new_df_columns)\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n            raise ValueError(\"OVERWRITE_SCHEMA não é suportado no UPSERT load type\")\n        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        # Constroi a condição de merge\n        merge_condition_parts = [f\"source.`{col}` = target.`{col}`\" for col in key_columns]\n        merge_condition = \" AND \".join(merge_condition_parts)\n\n        # Escreve a delta table\n        delta_table = DeltaTable.forPath(spark, location)\n        (\n            delta_table.alias(\"target\")\n            .merge(df.alias(\"source\"), merge_condition)\n            .whenMatchedUpdateAll()\n            .whenNotMatchedInsertAll()\n            .execute()\n        )\n\n        # Reset spark.conf\n        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", original_auto_merge)\n        \n    @classmethod     \n    def read_metadata(\n        cls, \n        df, \n        file_format: RawFileFormat,\n        table: str,\n        delimiter: str = ';',\n        extra_columns: list = [''],\n    ) -> DataFrame:\n\n        location   = f\"/mnt/bemoldigitalde/Trusted/DATA_DICTIONARY/{table}.csv\"\n                \n        ###try:\n          \n        df_metadata = (\n              spark.read\n              .format(file_format.lower())\n              .option(\"header\", True)\n              .option(\"escape\", \"\\\"\")\n              .option(\"mergeSchema\", True)\n              .option(\"delimiter\", delimiter)\n              .load(location)\n        )\n        \n        \n        # aplica regra somente colunas com o de/para e que podem ser apresentadas \n        df_metadata = df_metadata.where(\"TRUSTED_EXCLUIDO = 'False' and TRUSTED_COLUNA is not null\")\n        \n        # Verifica se existe colunas na Bronze com nome igual ao dicionario de dados que será aplicado na Silver  \n        ##field_list = []\n        Columns_Dropped = []\n        for field in df.schema.names:  \n          for linha in df_metadata.collect():\n              if linha[\"TRUSTED_COLUNA\"] == field and linha[\"TRUSTED_COLUNA\"] != linha[\"TRANSIENT_COLUNA\"]:\n                  #print(field + ' / ' + linha[\"TRUSTED_COLUNA\"] + ' / ' + linha[\"TRANSIENT_COLUNA\"])\n                  Columns_Dropped.append(field) \n                  df = df.drop(field)        \n        \n        # colunas enviadas como parametro para serem mantidas como \"DATA_EXTRACAO\"\n        Columns_to_keep = extra_columns     \n        \n        # aplica decode em cada coluna\n        for linha in df_metadata.collect():\n          \n          if linha[\"TRANSIENT_COLUNA\"] not in Columns_Dropped:\n                        \n            if  linha[\"ARQUIVO_DECODIFICADOR\"] is not None :\n                _table   =  linha[\"ARQUIVO_DECODIFICADOR\"]\n                _column  =  'XX_' + linha[\"TRANSIENT_COLUNA\"]\n\n                df_decode = spark.read.format(\"CSV\").option(\"header\", True)\\\n                      .option(\"escape\", \"\\\"\")\\\n                      .option(\"mergeSchema\", True)\\\n                      .option(\"delimiter\", \"|\")\\\n                      .load(f'/mnt/bemoldigitalde/Trusted/Arquivos_decodificacao/{_table}-DECODE.csv')\n\n                # retira duplicidade no arquivo de decode quando assim houver\n                df_decode.createOrReplaceTempView('tab_decode')  \n                df_decode = (spark.sql(\"SELECT code, description FROM tab_decode\"))                          \n                windowSpec  = Window.partitionBy(\"code\").orderBy(col(\"code\").desc())\n                df_decode = df_decode.withColumn(\"row_number\",row_number().over(windowSpec)).filter('row_number = 1').drop('row_number')\n\n                mapping = eval(linha['MAPEAMENTO_DECODIFICACAO'])\n                key_list = list(mapping.keys())\n                \n                # monta de/para de acordo com os campos no arquivo de decodificação\n                if   len(key_list) == 1:\n                    df = df.withColumn('key',df[key_list[0]]) \n                elif len(key_list) == 2:\n                    df = df.withColumn('key',concat(col(key_list[0]),lit('_'), col(key_list[1])))\n                elif len(key_list) == 3:\n                    df = df.withColumn('key',concat(col(key_list[0]),lit('_'), col(key_list[1]),lit('_'), col(key_list[2])))\n                elif len(key_list) == 4:\n                    df = df.withColumn('key',concat(col(key_list[0]),lit('_'), col(key_list[1]),lit('_'), col(key_list[2]),lit('_'), col(key_list[3])))\n                else :\n                    df = df.withColumn('key',concat(col(key_list[0]),lit('_'), col(key_list[1]),lit('_'), col(key_list[2]),lit('_'), col(key_list[3]),lit('_'), col(key_list[4])))\n\n                df = df.join(df_decode,(df.key == df_decode.code),how=\"left\") \n                df = df.drop('code')\n                df = df.drop('key')\n                df = df.withColumnRenamed('description',_column)\n        \n\n        for linha in df_metadata.collect():\n            \n            if linha[\"TRANSIENT_COLUNA\"] not in Columns_Dropped:\n              \n                # renomeia colunas decodificadas e remove colunas originais\n                if  linha[\"ARQUIVO_DECODIFICADOR\"] is not None :\n                    _column    =  'XX_' + linha[\"TRANSIENT_COLUNA\"]\n                    _transient =  linha[\"TRANSIENT_COLUNA\"]\n                    \n                    df = df.drop(_transient)\n                    df = df.withColumnRenamed(_column,_transient)\n                    \n                # renomeia a coluna\n                df = df.withColumnRenamed(linha[\"TRANSIENT_COLUNA\"],linha[\"TRUSTED_COLUNA\"])\n                Columns_to_keep.append(linha[\"TRUSTED_COLUNA\"])  \n                \n                column_type = linha[\"TRUSTED_TIPO\"]\n                column_name = linha[\"TRUSTED_COLUNA\"]\n                \n                # altera o type de cada coluna\n                if column_type == 'timestamp':\n                  df = df.withColumn(column_name, to_date(unix_timestamp(column_name, \"yyyyMMdd\").cast(column_type)))\n                elif column_type[:7] == 'decimal':\n                  if column_type[:10] == 'decimal_br': \n                    column_type = column_type.replace('_br','')\n                    decimal_br_udf = udf(decimal_br)\n                    df = df.withColumn(column_name, decimal_br_udf(column_name))\n                  int_part = int(column_type[8:-1].split(',')[0])\n                  decimal_places = int(column_type[8:-1].split(',')[1])\n                  df = df.withColumn(column_name, df[column_name].cast(DecimalType(int_part,decimal_places)))\n                else:\n                  df = df.withColumn(column_name, df[column_name].cast(column_type))   \n                                                  \n        #Remove colunas que não existe dicionario de dados                                       \n        df = df.select(Columns_to_keep)\n        \n        # insere zeros a esquerda na coluna ID_CLIENTE\n        if 'ID_CLIENTE' in df.columns:\n           df = df.withColumn('ID_CLIENTE', lpad(df['ID_CLIENTE'], 10, '0')) \n            \n        return df\n\n        ###except:\n\n        ###  print(f\"erro de leitura {table}\")\n\n        \n        \n        \n        \n#     @classmethod     \n#     def read_metadata(\n#         cls, \n#         df, \n#         file_format: RawFileFormat,\n#         table: str,\n#         delimiter: str = ';',\n#         extra_columns: list = [''],\n# ##        partition_key_check: str='', \n# ##        orderBy_key_check: str='',\n#     ) -> DataFrame:\n\n#         location   = f\"/mnt/bemoldigitalde/Trusted/DATA_DICTIONARY/{table}.csv\"\n                \n#         ###try:\n          \n#         df_metadata = (\n#               spark.read\n#               .format(file_format.lower())\n#               .option(\"header\", True)\n#               .option(\"escape\", \"\\\"\")\n#               .option(\"mergeSchema\", True)\n#               .option(\"delimiter\", delimiter)\n#               .load(location)\n#         )\n        \n        \n#         # aplica regra somente colunas com o de/para e que podem ser apresentadas \n#         df_metadata = df_metadata.where(\"TRUSTED_EXCLUIDO = 'False' and TRUSTED_COLUNA is not null\")\n        \n#         # Verifica se existe colunas na Bronze com nome igual ao dicionario de dados que será aplicado na Silver  \n#         ##field_list = []\n#         Columns_Dropped = []\n#         for field in df.schema.names:  \n#           for linha in df_metadata.collect():\n#               if linha[\"TRUSTED_COLUNA\"] == field and linha[\"TRUSTED_COLUNA\"] != linha[\"TRANSIENT_COLUNA\"]:\n#                   #print(field + ' / ' + linha[\"TRUSTED_COLUNA\"] + ' / ' + linha[\"TRANSIENT_COLUNA\"])\n#                   Columns_Dropped.append(field) \n#                   df = df.drop(field)        \n        \n#         # colunas enviadas como parametro para serem mantidas como \"DATA_EXTRACAO\"\n#         Columns_to_keep = extra_columns      \n\n#         for linha in df_metadata.collect():\n            \n#             if linha[\"TRANSIENT_COLUNA\"] not in Columns_Dropped:\n              \n#                 ## verifica se coluna partition by ou order by \n#  ##               if linha[\"TRANSIENT_COLUNA\"] == partition_key_check:\n#  ##                  partition_key_check = linha[\"TRUSTED_COLUNA\"]\n#  ##               if linha[\"TRANSIENT_COLUNA\"] == orderBy_key_check:\n#  ##                  orderBy_key_check = linha[\"TRUSTED_COLUNA\"]\n                    \n#                 # renomeia a coluna\n#                 df = df.withColumnRenamed(linha[\"TRANSIENT_COLUNA\"],linha[\"TRUSTED_COLUNA\"])\n#                 Columns_to_keep.append(linha[\"TRUSTED_COLUNA\"])  \n#                 # altera o type de cada coluna\n#                 column_type = linha[\"TRUSTED_TIPO\"]\n#                 column_name = linha[\"TRUSTED_COLUNA\"]\n#                 # altera o type de cada coluna\n#                 #if ( column_type == 'timestamp' ) & ( column_type is not None ) :\n#                 #if column_type == 'timestamp': \n#                 #  df = df.withColumn(column_name, when(substring(column_name,3,1) == '/', to_date(unix_timestamp(column_name, \"dd/mm/yyyy\").cast(column_type)) ).otherwise(to_date(unix_timestamp(column_name, \"yyyyMMdd\").cast(column_type)))) \n#                 if column_type == 'timestamp':\n#                   df = df.withColumn(column_name, to_date(unix_timestamp(column_name, \"yyyyMMdd\").cast(column_type)))\n#                 elif column_type[:7] == 'decimal':\n#                   if column_type[:10] == 'decimal_br': \n#                     column_type = column_type.replace('_br','')\n#                     decimal_br_udf = udf(decimal_br)\n#                     df = df.withColumn(column_name, decimal_br_udf(column_name))\n#                   int_part = int(column_type[8:-1].split(',')[0])\n#                   decimal_places = int(column_type[8:-1].split(',')[1])\n#                   df = df.withColumn(column_name, df[column_name].cast(DecimalType(int_part,decimal_places)))\n#                 else:\n#                   #print(linha[\"TRANSIENT_COLUNA\"] + ' / ' + linha[\"TRUSTED_COLUNA\"])\n#                   df = df.withColumn(column_name, df[column_name].cast(column_type))   \n\n#                 # aplica decode em cada coluna\n#                 if  linha[\"ARQUIVO_DECODIFICADOR\"] is not None:\n#                     decode_table   =  linha[\"ARQUIVO_DECODIFICADOR\"]\n#                     decode_column  =  linha[\"TRUSTED_COLUNA\"]\n\n#                     df_decode = spark.read.format(\"CSV\").option(\"header\", True)\\\n#                           .option(\"escape\", \"\\\"\")\\\n#                           .option(\"mergeSchema\", True)\\\n#                           .option(\"delimiter\", \"|\")\\\n#                           .load(f'/mnt/bemoldigitalde/Trusted/Arquivos_decodificacao/{decode_table}-DECODE.csv')\n\n#                     # retira duplicidade no arquivo de decode quando assim houver\n#                     df_decode.createOrReplaceTempView('tab_decode')  \n#                     df_decode = (spark.sql(\"SELECT * FROM tab_decode\"))                          \n#                     windowSpec  = Window.partitionBy(\"code\").orderBy(col(\"code\").desc())\n#                     df_decode = df_decode.withColumn(\"row_number\",row_number().over(windowSpec)).filter('row_number = 1').drop('row_number')\n\n                    \n#                     df = df.join(df_decode,(df[decode_column] == df_decode.code),how=\"left\") \n#                     df = df.drop('code')\n#                     df = df.drop(decode_column)\n#                     df = df.withColumnRenamed('description',decode_column)\n                    \n#         # Verifica se coluna de partition ou orderby tiveram alteração no nome             \n# ##        if partition_key_check != '':\n# ##            windowSpec  = Window.partitionBy(f\"{partition_key_check}\").orderBy(col(f\"{orderBy_key_check}\").desc())\n# ##            df = df.withColumn(\"row_number\",row_number().over(windowSpec)).filter('row_number = 1').drop('row_number')\n# ##        else:\n# ##            windowSpec  = Window.partitionBy(f\"{partition_key}\").orderBy(col(f\"{orderBy_key}\").desc())\n# ##            df = df.withColumn(\"row_number\",row_number().over(windowSpec)).filter('row_number = 1').drop('row_number')\n                    \n#         # Remove colunas que não existe dicionario de dados                                       \n#         df = df.select(Columns_to_keep)\n        \n#         # insere zeros a esquerda na coluna ID_CLIENTE\n#         if 'ID_CLIENTE' in df.columns:\n#            df = df.withColumn('ID_CLIENTE', lpad(df['ID_CLIENTE'], 10, '0')) \n\n# ##        df = df.distinct()     \n            \n#         return df\n\n#         ###except:\n\n#         ###  print(f\"erro de leitura {table}\")\n        \n\n    ####################################################\n    ## codigo importado funções já existentes Bemol   ##\n    ####################################################\n    \n    ### Lista arquivos disponiveis na landing\n    @classmethod        \n    def folder_read_path(\n        cls, \n        source_table, \n        source_path_landing,\n        years: List[str] = [],\n    ) -> list:\n      \n        ###try:\n        if years == []: \n          years = [file.name[:-1] for file in dbutils.fs.ls(source_path_landing)]\n        regex = '\\d+X*_\\d+X*_*\\d*.csv$'\n        file_list = []\n        for year in years:\n          #lista de arquivos nas pastas\n          filenames = dbutils.fs.ls(source_path_landing + '/' + year)\n          for file in filenames:\n            # obtenção do mês do arquivo\n            start, _ = re.search(regex, file.path).span()    #FALAR COM O SAULO DESSA BRUXARIA\n            month = file.path[start+4:start+6]\n            file_list.append(file.path)\n        return file_list \n        ###except :\n        ###    cls.exit_with_last_exception()\n            \n    ### Copia os arquivos da coletado para o diretório da processado\n    @staticmethod\n    def transport_collected_precessed(\n        file_list:List[str]):\n      \n        ###try:\n        for file in file_list:\n            dbutils.fs.mv(file, file.replace('Coletado', 'Processado'), True)  \n        ###except : \n        ###    print('Erro ao tentar mover os arquivos')\n          \n    ### Leitura da tabela Bronze já aplicando deduplicação \n    @staticmethod  \n    def read_bronze(\n        schema_bronze,\n        bronze_table, \n        partition_key, \n        orderBy_key, \n        start_datetime, \n        end_datetime,\n        from_to_field, \n    ) -> DataFrame:\n      \n        ###try:\n          \n        df = (spark.sql(f'''SELECT * FROM {schema_bronze}.{bronze_table}''')\n              .where((col(f\"{from_to_field}\") >= start_datetime) & (col(f\"{from_to_field}\") <= end_datetime) ))\n        windowSpec  = Window.partitionBy(f\"{partition_key}\").orderBy(col(f\"{orderBy_key}\").desc())\n        df = df.withColumn(\"row_number\",row_number().over(windowSpec)).filter('row_number = 1').drop('row_number')\n\n        return df\n          \n        ###except :\n        ###    print('Erro ao tentar selecionar tabela bronze')\n      \n    ### Transforma a moeda americana em br-pt\n    @staticmethod \n    def decimal_br(decimal):\n        if decimal == None:\n          return None\n        return decimal.replace('.','').replace(',','.')\n      "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef3904df-2ecf-4d03-9d04-b96346e58221","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Classes_1.1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3136467356617304}},"nbformat":4,"nbformat_minor":0}
