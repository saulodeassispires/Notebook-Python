{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafaa26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/08 11:34:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/08 11:34:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#import json\n",
    "from pyspark.sql import SparkSession, SQLContext, Row \n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"north_labs\").getOrCreate();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bcef7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cep errado ! 00011\n",
      "+-------+---------+-----------+---+----+-------+----------+-----------------+-----+---+\n",
      "| bairro|      cep|complemento|ddd| gia|   ibge|localidade|       logradouro|siafi| uf|\n",
      "+-------+---------+-----------+---+----+-------+----------+-----------------+-----+---+\n",
      "|Vila Ré|03667-000|           | 11|1004|3550308| São Paulo|Rua Baltazar Brum| 7107| SP|\n",
      "+-------+---------+-----------+---+----+-------+----------+-----------------+-----+---+\n",
      "\n",
      "+-------+---------+-----------+---+----+-------+----------+-----------------+-----+---+\n",
      "| bairro|      cep|complemento|ddd| gia|   ibge|localidade|       logradouro|siafi| uf|\n",
      "+-------+---------+-----------+---+----+-------+----------+-----------------+-----+---+\n",
      "|Vila Ré|03667-000|       null| 11|1004|3550308| São Paulo|Rua Baltazar Brum| 7107| SP|\n",
      "+-------+---------+-----------+---+----+-------+----------+-----------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ceps = ['03171060','11065201', '03667000', '00011']\n",
    "logradouro = []\n",
    "cidade = []\n",
    "\n",
    "##file = open('/tmp/test_northlabs.csv','a')\n",
    "### função para chamada da API\n",
    "def consulta_cep():\n",
    "    for cep in ceps:\n",
    "        if len(cep) == 8:\n",
    "            request = requests.get('https://viacep.com.br/ws/{}/json/'.format(cep))\n",
    "            cep_data = request.json()\n",
    "            #print(cep_data)         \n",
    "            #print(request.json())\n",
    "            #print('cidade {}'.format(cep_data['localidade']))\n",
    "            logradouro.append(cep_data['logradouro'] + \",\" + cep_data['localidade']) \n",
    "            cidade.append(cep_data['localidade'])\n",
    "\n",
    "            #dump         = [json.dumps(cep_data)]\n",
    "            #jsonRDD      = sc.parallelize(dump)\n",
    "            #resultado    = spark.read.json(jsonRDD)\n",
    "            #resultado.show()\n",
    "\n",
    "        else: \n",
    "            print(f'cep errado ! {cep}')\n",
    "\n",
    "    return cep_data\n",
    "\n",
    "df_json = consulta_cep()\n",
    "\n",
    "\n",
    "### gera o dataframe baseado na lista gerada durante a leitura do json/retorno da API\n",
    "df_cidade = spark.createDataFrame(cidade, StringType())\n",
    "\n",
    "### gera o data frame baseado no json recebido da API\n",
    "df = spark.createDataFrame([df_json])\n",
    "\n",
    "### cria uma tabela/view a partir do data frame\n",
    "df.createOrReplaceTempView('API')\n",
    "\n",
    "### executa query na tabela/view gerada \n",
    "df_select = spark.sql('select * from API')\n",
    "df_select.show()\n",
    "\n",
    "#grava csv porem em formato de subpasta com nome incorreto \n",
    "df_select.coalesce(1).write.format('csv').mode('overwrite').option(\"header\", \"true\").option(\"delimiter\",\",\").save('/tmp/test_northlabs')\n",
    "\n",
    "\n",
    "df_read = spark.read.format('csv').option('header', True).load('/tmp/test_northlabs')\n",
    "df_read.show()\n",
    "\n",
    "\n",
    "### ordena cidade e depois mostra somente top 3\n",
    "#select cidade, qtd from ( select cidade, count(*) as qtd from mawe_silver.clientes group by cidade )  order by 2 desc limit 3\n",
    "#ou \n",
    "#select * from (\n",
    "#select cidade, qtd, rank() over(order by qtd desc ) as ordenacao from (\n",
    "#select cidade, count(*) as qtd from mawe_silver.clientes group by cidade ) ) where ordenacao < 4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with open('/tmp/prime.txt', 'a') as prime_file:\n",
    "#f.write(\"\\nNew Line\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_select.write.csv(\"/tmp/df_select.csv\")\n",
    "#df_select.write.parquet(\"/tmp/df_select.parquet\")\n",
    "\n",
    "#df_select.write.format('csv').save('/tmp/north.csv')\n",
    "\n",
    "#df_withcol = data_rdd.toDF(['height','color','width'])\n",
    "\n",
    "#df_withcol = df_json.toDF(['coluna1','coluna2','coluna3','coluna4','coluna5','coluna6','coluna7','coluna8','coluna9','coluna10'])\n",
    "\n",
    "#df_cidade.write.csv(\"/tmp/df_cidade.csv\")\n",
    " \n",
    "\n",
    "##df_resultado = spark.read.json(cidade)\n",
    "\n",
    "#print(df_json)\n",
    "\n",
    "#df_leitura_jason = spark.read.json('/tmp/prime.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba76676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| bairro|\n",
      "+-------+\n",
      "|Vila Ré|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = spark.sql('select bairro from API')\n",
    "df_select.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7039b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import *\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"north_labs\").getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6827ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##df = spark.read.parquet('C:\\Saulo\\Ambev\\Arquivos Parquet\\ds_zz_Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac79f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_parquet = spark.sql('''create database if not exists db_northlabs ''')\n",
    "#df_parquet = spark.sql('''create table if not exists db_northlabs.InvoiceParquet using parquet location \"C:\\Saulo\\Ambev\\ds_zz_Product\"''')\n",
    "#df_parquet = spark.sql('''select distinct InvoiceNumber from db_parquet.InvoiceParquet  where InvoiceNumber <> 0536884625''')\n",
    "#df_parquet.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
