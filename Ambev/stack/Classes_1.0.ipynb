{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef3904df-2ecf-4d03-9d04-b96346e58221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "from enum import Enum, unique\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from types import TracebackType\n",
    "from typing import List, Type, TypedDict\n",
    "\n",
    "class Framework:\n",
    "    \n",
    "    @unique\n",
    "    class LoadType(str, Enum):\n",
    "        OVERWRITE_TABLE = \"OVERWRITE_TABLE\"\n",
    "        OVERWRITE_PARTITION = \"OVERWRITE_PARTITION\"\n",
    "        APPEND_ALL = \"APPEND_ALL\"  \n",
    "        APPEND_NEW = \"APPEND_NEW\"\n",
    "        UPSERT = \"UPSERT\"\n",
    "\n",
    "    @unique\n",
    "    class RawFileFormat(str, Enum):\n",
    "        PARQUET = \"PARQUET\"\n",
    "        DELTA = \"DELTA\"\n",
    "        ORC = \"ORC\"\n",
    "        CSV = \"CSV\"\n",
    "\n",
    "    @unique\n",
    "    class RunStatus(str, Enum):\n",
    "        SUCCEEDED = \"SUCCEEDED\"\n",
    "        FAILED = \"FAILED\"\n",
    "    \n",
    "    @unique\n",
    "    class SchemaEvolutionMode(str, Enum):\n",
    "        FAIL_ON_SCHEMA_MISMATCH = \"FAIL_ON_SCHEMA_MISMATCH\"\n",
    "        ADD_NEW_COLUMNS = \"ADD_NEW_COLUMNS\"\n",
    "        IGNORE_NEW_COLUMNS = \"IGNORE_NEW_COLUMNS\"\n",
    "        OVERWRITE_SCHEMA = \"OVERWRITE_SCHEMA\"\n",
    "\n",
    "\n",
    "    class ReturnObject(TypedDict):\n",
    "        status: str\n",
    "        target_object: str\n",
    "        num_records_read: int\n",
    "        num_records_loaded: int\n",
    "        num_records_errored_out: int\n",
    "        error_message: str\n",
    "        error_details: str\n",
    "\n",
    "    \n",
    "    def check_workspace_env()-> str:\n",
    "        ## busca workspace id\n",
    "        id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "        if id == 'xxxxxxxxx':\n",
    "            return  'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n",
    "        elif id == 'yyyyyyyyy':\n",
    "            return 'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n",
    "        elif id == 'zzzzzzzz':\n",
    "            return 'abfss://xxxxxxxx.dfs.core.windows.net/xxxxxxxx'\n",
    "        else:\n",
    "            ##raise ValueError(\"Este workspace não pertence a este escopo \")\n",
    "            return 'dbfs:/mnt/lakehouse/bronze'\n",
    "    \n",
    "    LAKEHOUSE_LANDING_ROOT =  check_workspace_env()\n",
    "    LAKEHOUSE_BRONZE_ROOT  =  check_workspace_env()\n",
    "    LAKEHOUSE_SILVER_ROOT  =  check_workspace_env()\n",
    "    LAKEHOUSE_GOLD_ROOT    =  check_workspace_env()\n",
    "    \n",
    "    ###print(LAKEHOUSE_LANDING_ROOT)\n",
    "    \n",
    "    @classmethod\n",
    "    def _build_return_object(\n",
    "        cls,\n",
    "        status: RunStatus,\n",
    "        target_object: str,\n",
    "        num_records_read: int = 0,\n",
    "        num_records_loaded: int = 0,\n",
    "        error_message: str = \"\",\n",
    "        error_details: str = \"\",\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\" Retorna objeto com detalhes da ultima excução/erro\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"target_object\": target_object,\n",
    "            \"num_records_read\": num_records_read,\n",
    "            \"num_records_loaded\": num_records_loaded,\n",
    "            \"num_records_errored_out\": num_records_read - num_records_loaded,\n",
    "            \"error_message\": error_message[:8000],\n",
    "            \"error_details\": error_details,\n",
    "        }\n",
    "                \n",
    "    @classmethod\n",
    "    def exit_with_object(cls, results: ReturnObject):\n",
    "        \"\"\" Retorno de objeto com o resultado da execução \n",
    "        \"\"\"\n",
    "        dbutils.notebook.exit(json.dumps(results))\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def exit_with_last_exception(cls):\n",
    "        \"\"\"Busca o ultimo erro e retorno um objeto.\n",
    "        \"\"\"\n",
    "        exc_type, exc_value, _ = sys.exc_info()\n",
    "        results = cls._build_return_object(\n",
    "            status=cls.RunStatus.FAILED,\n",
    "            target_object=None,\n",
    "            error_message=f\"{exc_type.__name__}: {exc_value}\",\n",
    "            error_details=traceback.format_exc(),\n",
    "        )\n",
    "        cls.exit_with_object(results)\n",
    "        \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def read_landing_zone_dataframe(\n",
    "        cls,\n",
    "        file_format: RawFileFormat,\n",
    "        location: str,\n",
    "        delimiter: str = ';',\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Leitura de arquivo da camada Landing com opções de parametros como csv, parquet, avro \"\"\"\n",
    "\n",
    "        try:\n",
    "            df = (\n",
    "                spark.read\n",
    "                .format(file_format.lower())\n",
    "                .option(\"header\", True)\n",
    "                .option(\"escape\", \"\\\"\")\n",
    "                .option(\"mergeSchema\", True)\n",
    "                .option(\"delimiter\", delimiter)\n",
    "                .load(location)\n",
    "            )\n",
    "\n",
    "            # transforma todas as colunas para string\n",
    "            if file_format != cls.RawFileFormat.CSV:\n",
    "                non_string_columns = [col for col, dtype in df.dtypes if dtype != \"string\"]\n",
    "                for column in non_string_columns:\n",
    "                    df = df.withColumn(column, F.col(column).cast(\"string\"))\n",
    "\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "            \n",
    "    @classmethod\n",
    "    def generate_bronze_table_location(\n",
    "        cls,\n",
    "        table_name: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Cria caminho para tabela bronze \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Verifica se os parametro tem algum campo nulo ou em branco\n",
    "            params_list = [schema_name, table_name]\n",
    "            if any(len(x) == 0 for x in params_list):\n",
    "                raise ValueError(\"Caminho não pode conter brancos ou nulo, verifique !\")\n",
    "            return f\"{cls.LAKEHOUSE_BRONZE_ROOT}/{table_name}/\"\n",
    "\n",
    "            \n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def write_delta_table(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        schema_name: str,\n",
    "        table_name: str,\n",
    "        load_type: LoadType,\n",
    "        key_columns: List[str] = [],\n",
    "        partition_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "      \n",
    "        \"\"\"Escreve um DataFrame como Delta Table \n",
    "        \"\"\"\n",
    "        num_records_read = 0\n",
    "        num_records_loaded = 0\n",
    "        #print('dentro da classe')\n",
    "        #print(location)\n",
    "        #print(schema_name)\n",
    "        #print(table_name)\n",
    "        #print(load_type)\n",
    "        \n",
    "      \n",
    "        try:\n",
    "            # Tabela deve existir para aplicar o merge\n",
    "            if load_type != cls.LoadType.APPEND_ALL and not DeltaTable.isDeltaTable(spark, location):\n",
    "                print(\"Delta table ainda não existe. Altere load_type para APPEND_ALL e reexecute\")\n",
    "                load_type = cls.LoadType.APPEND_ALL\n",
    "\n",
    "            # Otimiza a escrita para evitar small files \n",
    "            spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n",
    "\n",
    "            # Determina o load type\n",
    "            if load_type == cls.LoadType.APPEND_ALL:\n",
    "                \n",
    "                cls._write_table_using_append_all(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    partition_columns=partition_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            elif load_type == cls.LoadType.UPSERT:\n",
    "                if len(key_columns) == 0:\n",
    "                    raise ValueError(\"Nenhuma coluna foi especificada para o upsert\")\n",
    "\n",
    "                cls._write_table_using_upsert(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    key_columns=key_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            # Cria Metadados e tabela\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name};\")\n",
    "            spark.sql(f\"CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} USING DELTA LOCATION '{location}';\")\n",
    "\n",
    "            return cls._build_return_object(\n",
    "                status=cls.RunStatus.SUCCEEDED,\n",
    "                target_object=f\"{schema_name}.{table_name}\",\n",
    "                num_records_read=num_records_read,\n",
    "                num_records_loaded=num_records_loaded,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return cls._build_return_object(\n",
    "                status=cls.RunStatus.FAILED,\n",
    "                target_object=f\"{schema_name}.{table_name}\",\n",
    "                num_records_read=num_records_read,\n",
    "                num_records_loaded=num_records_loaded,\n",
    "                error_message=str(e),\n",
    "                error_details=traceback.format_exc(),\n",
    "            )\n",
    "            \n",
    "    @classmethod\n",
    "    def _write_table_using_append_all(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        partition_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Escreve o DataFrame utilizando APPEND_ALL.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_writer = (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "        )\n",
    "\n",
    "        # Checa se havera partições\n",
    "        if len(partition_columns) > 0:\n",
    "            df_writer = df_writer.partitionBy(partition_columns)\n",
    "\n",
    "        # verifica schema evolution\n",
    "        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n",
    "            pass\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n",
    "            df_writer = df_writer.option(\"mergeSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n",
    "            if DeltaTable.isDeltaTable(spark, location):\n",
    "                table_columns = DeltaTable.forPath(spark, location).columns\n",
    "                new_df_columns = [col for col in df.columns if col not in table_columns]\n",
    "                df = df.drop(*new_df_columns)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n",
    "            df_writer = df_writer.option(\"overwriteSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Grava a Delta Table\n",
    "        df_writer.save(location)\n",
    "        \n",
    "    ############################################\n",
    "    ## codigo importado funções já existentes ##\n",
    "    ############################################\n",
    "    @classmethod        \n",
    "    def folder_read_path(\n",
    "        cls, \n",
    "        source_table, \n",
    "        source_path_landing\n",
    "    ) -> list:\n",
    "      \n",
    "        try:\n",
    "            years = [file.name[:-1] for file in dbutils.fs.ls(source_path_landing)]\n",
    "            regex = '\\d+X*_\\d+X*_*\\d*.csv$'\n",
    "            file_list = []\n",
    "            for year in years:\n",
    "              #lista de arquivos nas pastas\n",
    "              filenames = dbutils.fs.ls(source_path_landing + '/' + year)\n",
    "              for file in filenames:\n",
    "                # obtenção do mês do arquivo\n",
    "                start, _ = re.search(regex, file.path).span()\n",
    "                month = file.path[start+4:start+6]\n",
    "                file_list.append(file.path)\n",
    "                for item in file_list:\n",
    "                  print(item)\n",
    "              ####df = spark.read.format('csv').options(header='true').option('delimiter', '|').load(file_list)\n",
    "            return file_list \n",
    "        except :\n",
    "            cls.exit_with_last_exception()\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c09abc2-aa3c-4b11-8a31-812b6bb4c6ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Classes_1.0",
   "notebookOrigID": 3749574210653413,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
