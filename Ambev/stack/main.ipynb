{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abad0f0d-53bb-4bb5-a62d-f46a85268f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "from enum import Enum, unique\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from types import TracebackType\n",
    "from typing import List, Type, TypedDict\n",
    "\n",
    "class Framework:\n",
    "    \"\"\"Reusable functions for all BrewDat projects.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    read_landing_zone_dataframe -> DataFrame\n",
    "        Read a DataFrame from the Landing Zone.\n",
    "    clean_column_names -> DataFrame\n",
    "        Normalize the name of all the columns in a given DataFrame.\n",
    "    create_or_replace_business_key_column -> DataFrame\n",
    "        Create a standard business key concatenating multiple columns.\n",
    "    create_or_replace_audit_columns -> DataFrame\n",
    "        Create or replace BrewDat audit columns in the given DataFrame.\n",
    "    deduplicate_records -> DataFrame\n",
    "        Deduplicate rows from a DataFrame using key and watermark columns.\n",
    "    drop_empty_columns -> DataFrame\n",
    "        Drop columns which are null or empty for all the rows in the DataFrame.\n",
    "    generate_bronze_table_location -> str\n",
    "        Build the standard location for a Bronze table.\n",
    "    generate_silver_table_location -> str\n",
    "        Build the standard location for a Silver table.\n",
    "    generate_gold_table_location -> str\n",
    "        Build the standard location for a Gold table.\n",
    "    write_delta_table -> ReturnObject\n",
    "        Write the DataFrame as a delta table.\n",
    "    exit_with_object\n",
    "        Finish execution returning an object to the notebook's caller.\n",
    "    exit_with_last_exception\n",
    "        Handle the last unhandled exception, returning an object to the notebook's caller.\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    # Constants, enums, and helper classes #\n",
    "    ########################################\n",
    "    \n",
    "    def check_workspace_env()-> str:\n",
    "        ## get workspace id\n",
    "        id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "        if id == '1967032416258666':\n",
    "            return 'dev'\n",
    "        elif id == '5424328937982531':\n",
    "            return 'qa'\n",
    "        elif id == '2701096371086075':\n",
    "            return 'prod'\n",
    "        else:\n",
    "            raise ValueError(\"This workspace doesn't belongs to framework bees scope\")\n",
    "            \n",
    "    \n",
    "    ENV = check_workspace_env()\n",
    "    \n",
    "    LAKEHOUSE_LANDING_ROOT = f\"abfss://bees-plz@brewdatblobsagb{ENV}.dfs.core.windows.net\" \n",
    "    LAKEHOUSE_BRONZE_ROOT =  f\"abfss://brewdat-ghq@brewdatadlsgb{ENV}.dfs.core.windows.net/root/lz_data/non_src_sys/bees_engine\"\n",
    "    LAKEHOUSE_SILVER_ROOT =  f\"abfss://brewdat-ghq@brewdatadlsgb{ENV}.dfs.core.windows.net/root/hz_data/non_src_sys/bees_engine\"\n",
    "    LAKEHOUSE_GOLD_ROOT =  f\"abfss://brewdat-ghq@brewdatadlsgb{ENV}.dfs.core.windows.net/root/tz_data/non_src_sys/bees_engine\"\n",
    "    \n",
    "    \n",
    "    @unique\n",
    "    class LoadType(str, Enum):\n",
    "        OVERWRITE_TABLE = \"OVERWRITE_TABLE\"\n",
    "        OVERWRITE_PARTITION = \"OVERWRITE_PARTITION\"\n",
    "        APPEND_ALL = \"APPEND_ALL\"  # only for Bronze tables, as it is bad for backfilling\n",
    "        APPEND_NEW = \"APPEND_NEW\"\n",
    "        UPSERT = \"UPSERT\"\n",
    "        TYPE_2_SCD = \"TYPE_2_SCD\"  # not yet implemented\n",
    "\n",
    "\n",
    "    @unique\n",
    "    class RawFileFormat(str, Enum):\n",
    "        PARQUET = \"PARQUET\"\n",
    "        DELTA = \"DELTA\"\n",
    "        ORC = \"ORC\"\n",
    "        CSV = \"CSV\"\n",
    "\n",
    "\n",
    "    @unique\n",
    "    class SchemaEvolutionMode(str, Enum):\n",
    "        FAIL_ON_SCHEMA_MISMATCH = \"FAIL_ON_SCHEMA_MISMATCH\"\n",
    "        ADD_NEW_COLUMNS = \"ADD_NEW_COLUMNS\"\n",
    "        IGNORE_NEW_COLUMNS = \"IGNORE_NEW_COLUMNS\"\n",
    "        OVERWRITE_SCHEMA = \"OVERWRITE_SCHEMA\"\n",
    "        RESCUE_NEW_COLUMNS = \"RESCUE_NEW_COLUMNS\"  # not yet implemented\n",
    "\n",
    "\n",
    "    @unique\n",
    "    class RunStatus(str, Enum):\n",
    "        SUCCEEDED = \"SUCCEEDED\"\n",
    "        FAILED = \"FAILED\"\n",
    "\n",
    "\n",
    "    class ReturnObject(TypedDict):\n",
    "        status: str\n",
    "        target_object: str\n",
    "        num_records_read: int\n",
    "        num_records_loaded: int\n",
    "        num_records_errored_out: int\n",
    "        error_message: str\n",
    "        error_details: str\n",
    "\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # Public methods #\n",
    "    ##################\n",
    "\n",
    "    @classmethod\n",
    "    def read_landing_zone_dataframe(\n",
    "        cls,\n",
    "        file_format: RawFileFormat,\n",
    "        location: str,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Read a DataFrame from the Landing Zone.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_format : RawFileFormat\n",
    "            The raw file format use in this dataset (CSV, PARQUET, etc.).\n",
    "        location : str\n",
    "            Absolute Data Lake path for the physical location of this dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            The PySpark DataFrame read from the Landing Zone.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = (\n",
    "                spark.read\n",
    "                .format(file_format.lower())\n",
    "                .option(\"header\", True)\n",
    "                .option(\"escape\", \"\\\"\")\n",
    "                .option(\"mergeSchema\", True)\n",
    "                .load(location)\n",
    "            )\n",
    "\n",
    "            # Cast everything to string\n",
    "            if file_format != cls.RawFileFormat.CSV:\n",
    "                non_string_columns = [col for col, dtype in df.dtypes if dtype != \"string\"]\n",
    "                for column in non_string_columns:\n",
    "                    df = df.withColumn(column, F.col(column).cast(\"string\"))\n",
    "\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def clean_column_names(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        except_for: List[str] = [],\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Normalize the name of all the columns in a given DataFrame.\n",
    "\n",
    "        Uses BrewDat's standard approach as seen in other Notebooks.\n",
    "        Improved to also trim (strip) underscores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            The PySpark DataFrame to modify.\n",
    "        except_for : List[str], default=[]\n",
    "            A list of column names that should NOT be modified.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            The modified PySpark DataFrame with renamed columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            column_names = df.schema.names\n",
    "            for column_name in column_names:\n",
    "                if column_name in except_for:\n",
    "                    continue  # Skip\n",
    "\n",
    "                # \\W is \"anything that is not alphanumeric or underscore\"\n",
    "                # Equivalent to [^A-Za-z0-9_]\n",
    "                new_column_name = re.sub(\"\\W+\", \"_\", column_name.strip().lower())\n",
    "                if column_name != new_column_name:\n",
    "                    df = df.withColumnRenamed(column_name, new_column_name)\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create_or_replace_business_key_column(\n",
    "        df: DataFrame,\n",
    "        business_key_column_name: str,\n",
    "        key_columns: List[str],\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Create a standard business key concatenating multiple columns.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            The PySpark DataFrame to modify.\n",
    "        business_key_column_name : str\n",
    "            The name of the concatenated business key column.\n",
    "        key_columns : List[str]\n",
    "            The names of the columns used to uniquely identify each record the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            The PySpark DataFrame with the desired business key.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(key_columns) == 0:\n",
    "                raise ValueError(\"No key column was given\")\n",
    "\n",
    "            # Check for NULL values\n",
    "            filter_clauses = [f\"`{key_column}` IS NULL\" for key_column in key_columns]\n",
    "            filter_string = \" OR \".join(filter_clauses)\n",
    "            if df.filter(filter_string).limit(1).count() > 0:\n",
    "                raise ValueError(\"Business key would contain null values.\")\n",
    "\n",
    "            df = df.withColumn(business_key_column_name, F.lower(F.concat_ws(\"__\", *key_columns)))\n",
    "\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create_or_replace_audit_columns(cls, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Create or replace BrewDat audit columns in the given DataFrame.\n",
    "\n",
    "        The following audit columns are created/replaced:\n",
    "            lakehouse_insert_gmt_ts: timestamp of when the record was inserted.\n",
    "            lakehouse_update_gmt_ts: timestamp of when the record was last updated.\n",
    "            lakehouse_load_job_id: id of the job that loaded/updated the record.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            The PySpark DataFrame to modify.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            The modified PySpark DataFrame with audit columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get current timestamp\n",
    "            current_timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Get Databricks' jobId for this notebook run\n",
    "            notebook_context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "            notebook_job_id = None\n",
    "            if notebook_context.jobId().nonEmpty():\n",
    "                notebook_job_id = notebook_context.jobId().value()\n",
    "\n",
    "            # Create or replace columns\n",
    "            if \"lakehouse_insert_gmt_ts\" in df.columns:\n",
    "                df = df.fillna(current_timestamp, \"lakehouse_insert_gmt_ts\")\n",
    "            else:\n",
    "                df = df.withColumn(\"lakehouse_insert_gmt_ts\", F.lit(current_timestamp).cast(\"timestamp\"))\n",
    "            df = df.withColumn(\"lakehouse_update_gmt_ts\", F.lit(current_timestamp).cast(\"timestamp\"))\n",
    "            df = df.withColumn(\"lakehouse_load_job_id\", F.lit(notebook_job_id).cast(\"string\"))\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def deduplicate_records(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        key_columns: List[str],\n",
    "        watermark_column: str,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Deduplicate rows from a DataFrame using key and watermark columns.\n",
    "\n",
    "        We do not use orderBy followed by dropDuplicates because it\n",
    "        would require a coalesce(1) to preserve the order of the rows.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            The PySpark DataFrame to modify.\n",
    "        key_columns : List[str]\n",
    "            The names of the columns used to uniquely identify each record the table.\n",
    "        watermark_column : str\n",
    "            The name of a datetime column used to select the newest records.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            The deduplicated PySpark DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(key_columns) == 0:\n",
    "                raise ValueError(\"No key column was given\")\n",
    "\n",
    "            return (\n",
    "                df\n",
    "                .withColumn(\"__dedup_row_number\", F.row_number().over(\n",
    "                    Window.partitionBy(*key_columns).orderBy(F.col(watermark_column).desc())\n",
    "                ))\n",
    "                .filter(\"__dedup_row_number = 1\")\n",
    "                .drop(\"__dedup_row_number\")\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def drop_empty_columns(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        except_for: List[str] = [],\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Drop columns which are null or empty for all the rows in the DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            The PySpark DataFrame to modify.\n",
    "        except_for : List[str], default=[]\n",
    "            A list of column names that should NOT be dropped.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            The modified PySpark DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            non_empty_counts = df.selectExpr([\n",
    "                f\"COUNT(`{col}`) AS `{col}`\" if datatype != \"string\"\n",
    "                else f\"COUNT(NULLIF(NULLIF(NULLIF(NULLIF(`{col}`, ''), ' '), 'null'), 'NULL')) AS `{col}`\"\n",
    "                for col, datatype in df.dtypes\n",
    "                if col not in except_for\n",
    "            ]).first().asDict()\n",
    "            null_columns = [col for col, count in non_empty_counts.items() if count == 0]\n",
    "            df = df.drop(*null_columns)\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def generate_bronze_table_location(\n",
    "        cls,\n",
    "        source_zone:str,\n",
    "        source_sys_name:str,\n",
    "        source_business_domain: str,\n",
    "        table_name: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Build the standard location for a Bronze table.\n",
    "        Parameters\n",
    "        ----------\n",
    "        source_zone : str\n",
    "            Zone of the source system.\n",
    "        source_system_name : str\n",
    "            Name of the source system.\n",
    "        source_business_domain : str\n",
    "            Business domain of the source system.\n",
    "        table_name : str\n",
    "            Name of the target table_name.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Standard location for the delta table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check that no parameter is None or empty string\n",
    "            params_list = [source_zone, source_sys_name, source_business_domain, table_name]\n",
    "            if any(len(x) == 0 for x in params_list):\n",
    "                raise ValueError(\"Location would contain null or empty values.\")\n",
    "\n",
    "            return f\"{cls.LAKEHOUSE_BRONZE_ROOT}/{source_zone}/{source_sys_name}/{source_business_domain}/{table_name}\"\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def generate_silver_table_location(\n",
    "        cls,\n",
    "        source_zone:str,\n",
    "        source_sys_name:str,\n",
    "        source_business_domain: str,\n",
    "        table_name: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Build the standard location for a Bronze table.\n",
    "        Parameters\n",
    "        ----------\n",
    "        source_zone : str\n",
    "            Zone of the source system.\n",
    "        source_system_name : str\n",
    "            Name of the source system.\n",
    "        source_business_domain : str\n",
    "            Business domain of the source system.\n",
    "        table_name : str\n",
    "            Name of the target table_name.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Standard location for the delta table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check that no parameter is None or empty string\n",
    "            params_list = [source_zone, source_sys_name, source_business_domain, table_name]\n",
    "            if any(len(x) == 0 for x in params_list):\n",
    "                raise ValueError(\"Location would contain null or empty values.\")\n",
    "\n",
    "            return f\"{cls.LAKEHOUSE_SILVER_ROOT}/{source_zone}/{source_sys_name}/{source_business_domain}/{table_name}\"\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "    @classmethod\n",
    "    def generate_gold_table_location(\n",
    "        cls,\n",
    "        target_zone: str,\n",
    "        target_business_domain: str,\n",
    "        project: str,\n",
    "        table_name: str\n",
    "    ) -> str:\n",
    "        \"\"\"Build the standard location for a Gold table.\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_zone : str\n",
    "            Zone of the resulting table_name.\n",
    "        target_business_domain : str\n",
    "            Business domain of the resulting table_name.\n",
    "        project : str\n",
    "            Name of the project.\n",
    "        table_name : str\n",
    "            Name of the target dataset.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Standard location for the delta table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check that no parameter is None or empty string\n",
    "            params_list = [target_zone, target_business_domain, project, table_name]\n",
    "            if any(len(x) == 0 for x in params_list):\n",
    "                raise ValueError(\"Location would contain null or empty values.\")\n",
    "\n",
    "            return f\"{cls.LAKEHOUSE_GOLD_ROOT}/{target_zone}/{target_business_domain}/{project}/{table_name}\"\n",
    "\n",
    "        except:\n",
    "            cls.exit_with_last_exception()\n",
    "\n",
    "    @classmethod\n",
    "    def write_delta_table(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        schema_name: str,\n",
    "        table_name: str,\n",
    "        load_type: LoadType,\n",
    "        key_columns: List[str] = [],\n",
    "        partition_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Write the DataFrame as a delta table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to modify.\n",
    "        location : str\n",
    "            Absolute Delta Lake path for the physical location of this delta table.\n",
    "        schema_name : str\n",
    "            Name of the schema/database for the table in the metastore.\n",
    "            Schema is created if it does not exist.\n",
    "        table_name : str\n",
    "            Name of the table in the metastore.\n",
    "        load_type : Framework.LoadType\n",
    "            Specifies the way in which the table should be loaded.\n",
    "            OVERWRITE_TABLE: the entire table is rewritten in every execution.\n",
    "                Avoid whenever possible, as this is not good for large tables.\n",
    "                This deletes records that are not present in df.\n",
    "            OVERWRITE_PARTITION: overwrite a single partition based on partitionColumns.\n",
    "                This deletes records that are not present in df for the chosen partition.\n",
    "                The df must be filtered such that it contains a single partition.\n",
    "            APPEND_NEW: write new records in the df to the existing table.\n",
    "                Records for which the key already exists in the table are ignored.\n",
    "            UPSERT: write new records and update existing records based on the key.\n",
    "                This does NOT delete existing records that are not included in df.\n",
    "            TYPE_2_SCD: use the standard type-2 Slowly Changing Dimension implementation.\n",
    "                This essentially uses an upsert that keeps track of all previous versions of each record.\n",
    "                For more information: https://en.wikipedia.org/wiki/Slowly_changing_dimension\n",
    "        key_columns : List[str], default=[]\n",
    "            The names of the columns used to uniquely identify each record the table.\n",
    "            Used for APPEND_NEW, UPSERT, and TYPE_2_SCD load types.\n",
    "        partition_columns : List[str], default=[]\n",
    "            The names of the columns used to partition the table.\n",
    "        schema_evolution_mode : Framework.SchemaEvolutionMode, default=ADD_NEW_COLUMNS\n",
    "            Specifies the way in which schema mismatches should be handled.\n",
    "            FAIL_ON_SCHEMA_MISMATCH: fail if the table's schema is not compatible with the DataFrame's.\n",
    "                This is the default Spark behavior when no option is given.\n",
    "            ADD_NEW_COLUMNS: schema evolution through adding new columns to the target table.\n",
    "                This is the same as using the option \"mergeSchema\".\n",
    "            IGNORE_NEW_COLUMNS: drop DataFrame columns that do not exist in the table's schema.\n",
    "                Does nothing if the table does not yet exist in the Hive metastore.\n",
    "            OVERWRITE_SCHEMA: overwrite the table's schema with the DataFrame's schema.\n",
    "                This is the same as using the option \"overwriteSchema\".\n",
    "            RESCUE_NEW_COLUMNS: Create a new struct-type column to collect data for new columns.\n",
    "                This is the same strategy used in AutoLoader's rescue mode.\n",
    "                For more information: https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-evolution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        num_records_read = 0\n",
    "        num_records_loaded = 0\n",
    "\n",
    "        try:\n",
    "            # Count source records\n",
    "            num_records_read = df.count()\n",
    "\n",
    "            # Table must exist if we are merging data\n",
    "            if load_type != cls.LoadType.APPEND_ALL and not DeltaTable.isDeltaTable(spark, location):\n",
    "                print(\"Delta table does not exist yet. Setting load_type to APPEND_ALL for this run.\")\n",
    "                load_type = cls.LoadType.APPEND_ALL\n",
    "\n",
    "            # Use optimized writes to create less small files\n",
    "            spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n",
    "\n",
    "            # Set load_type options\n",
    "            if load_type == cls.LoadType.OVERWRITE_TABLE:\n",
    "                if num_records_read == 0:\n",
    "                    raise ValueError(\"Attempted to overwrite a table with an empty dataset. Operation aborted.\")\n",
    "\n",
    "                cls._write_table_using_overwrite_table(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    partition_columns=partition_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            elif load_type == cls.LoadType.OVERWRITE_PARTITION:\n",
    "                if len(partition_columns) == 0:\n",
    "                    raise ValueError(\"No partition column was given\")\n",
    "\n",
    "                if num_records_read == 0:\n",
    "                    raise ValueError(\"Attempted to overwrite a partition with an empty dataset. Operation aborted.\")\n",
    "\n",
    "                cls._write_table_using_overwrite_partition(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    partition_columns=partition_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            elif load_type == cls.LoadType.APPEND_ALL:\n",
    "                cls._write_table_using_append_all(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    partition_columns=partition_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            elif load_type == cls.LoadType.APPEND_NEW:\n",
    "                if len(key_columns) == 0:\n",
    "                    raise ValueError(\"No key column was given\")\n",
    "\n",
    "                cls._write_table_using_append_new(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    key_columns=key_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            elif load_type == cls.LoadType.UPSERT:\n",
    "                if len(key_columns) == 0:\n",
    "                    raise ValueError(\"No key column was given\")\n",
    "\n",
    "                cls._write_table_using_upsert(\n",
    "                    df=df,\n",
    "                    location=location,\n",
    "                    key_columns=key_columns,\n",
    "                    schema_evolution_mode=schema_evolution_mode,\n",
    "                )\n",
    "            elif load_type == cls.LoadType.TYPE_2_SCD:\n",
    "                if len(key_columns) == 0:\n",
    "                    raise ValueError(\"No key column was given\")\n",
    "\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            # Find out how many records we have just written\n",
    "            num_version_written = spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")\n",
    "            delta_table = DeltaTable.forPath(spark, location)\n",
    "            history_df = (\n",
    "                delta_table.history()\n",
    "                .filter(f\"version = {num_version_written}\")\n",
    "                .select(F.col(\"operationMetrics.numOutputRows\").cast(\"int\"))\n",
    "            )\n",
    "            num_records_loaded = history_df.first()[0]\n",
    "\n",
    "            # Create the Hive database and table\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name};\")\n",
    "            spark.sql(f\"CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} USING DELTA LOCATION '{location}';\")\n",
    "\n",
    "            return cls._build_return_object(\n",
    "                status=cls.RunStatus.SUCCEEDED,\n",
    "                target_object=f\"{schema_name}.{table_name}\",\n",
    "                num_records_read=num_records_read,\n",
    "                num_records_loaded=num_records_loaded,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return cls._build_return_object(\n",
    "                status=cls.RunStatus.FAILED,\n",
    "                target_object=f\"{schema_name}.{table_name}\",\n",
    "                num_records_read=num_records_read,\n",
    "                num_records_loaded=num_records_loaded,\n",
    "                error_message=str(e),\n",
    "                error_details=traceback.format_exc(),\n",
    "            )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def exit_with_object(cls, results: ReturnObject):\n",
    "        \"\"\"Finish execution returning an object to the notebook's caller.\n",
    "\n",
    "        Used to return the results of a write operation to the orchestrator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results : ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        dbutils.notebook.exit(json.dumps(results))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def exit_with_last_exception(cls):\n",
    "        \"\"\"Handle the last unhandled exception, returning an object to the notebook's caller.\n",
    "\n",
    "            The most recent exception is obtained from sys.exc_info().\n",
    "\n",
    "            Examples\n",
    "            --------\n",
    "            try:\n",
    "                # some code\n",
    "            except:\n",
    "                Framework.exit_with_last_exception()\n",
    "        \"\"\"\n",
    "        exc_type, exc_value, _ = sys.exc_info()\n",
    "        results = cls._build_return_object(\n",
    "            status=cls.RunStatus.FAILED,\n",
    "            target_object=None,\n",
    "            error_message=f\"{exc_type.__name__}: {exc_value}\",\n",
    "            error_details=traceback.format_exc(),\n",
    "        )\n",
    "        cls.exit_with_object(results)\n",
    "\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # Private methods #\n",
    "    ###################\n",
    "\n",
    "    @classmethod\n",
    "    def _write_table_using_overwrite_table(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        partition_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Write the DataFrame using OVERWRITE_TABLE.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to modify.\n",
    "        location : str\n",
    "            Absolute Delta Lake path for the physical location of this delta table.\n",
    "        partition_columns : List[str], default=[]\n",
    "            The names of the columns used to partition the table.\n",
    "        schema_evolution_mode : Framework.SchemaEvolutionMode, default=ADD_NEW_COLUMNS\n",
    "            Specifies the way in which schema mismatches should be handled.\n",
    "            FAIL_ON_SCHEMA_MISMATCH: fail if the table's schema is not compatible with the DataFrame's.\n",
    "                This is the default Spark behavior when no option is given.\n",
    "            ADD_NEW_COLUMNS: schema evolution through adding new columns to the target table.\n",
    "                This is the same as using the option \"mergeSchema\".\n",
    "            IGNORE_NEW_COLUMNS: drop DataFrame columns that do not exist in the table's schema.\n",
    "                Does nothing if the table does not yet exist in the Hive metastore.\n",
    "            OVERWRITE_SCHEMA: overwrite the table's schema with the DataFrame's schema.\n",
    "                This is the same as using the option \"overwriteSchema\".\n",
    "            RESCUE_NEW_COLUMNS: Create a new struct-type column to collect data for new columns.\n",
    "                This is the same strategy used in AutoLoader's rescue mode.\n",
    "                For more information: https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-evolution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        df_writer = (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "        )\n",
    "\n",
    "        # Set partition options\n",
    "        if len(partition_columns) > 0:\n",
    "            df_writer = df_writer.partitionBy(partition_columns)\n",
    "\n",
    "        # Set schema_evolution_mode options\n",
    "        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n",
    "            pass\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n",
    "            df_writer = df_writer.option(\"mergeSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n",
    "            if DeltaTable.isDeltaTable(spark, location):\n",
    "                table_columns = DeltaTable.forPath(spark, location).columns\n",
    "                new_df_columns = [col for col in df.columns if col not in table_columns]\n",
    "                df = df.drop(*new_df_columns)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n",
    "            df_writer = df_writer.option(\"overwriteSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Write to the delta table\n",
    "        df_writer.save(location)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _write_table_using_overwrite_partition(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        partition_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Write the DataFrame using OVERWRITE_PARTITION.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to modify.\n",
    "        location : str\n",
    "            Absolute Delta Lake path for the physical location of this delta table.\n",
    "        partition_columns : List[str], default=[]\n",
    "            The names of the columns used to partition the table.\n",
    "        schema_evolution_mode : Framework.SchemaEvolutionMode, default=ADD_NEW_COLUMNS\n",
    "            Specifies the way in which schema mismatches should be handled.\n",
    "            FAIL_ON_SCHEMA_MISMATCH: fail if the table's schema is not compatible with the DataFrame's.\n",
    "                This is the default Spark behavior when no option is given.\n",
    "            ADD_NEW_COLUMNS: schema evolution through adding new columns to the target table.\n",
    "                This is the same as using the option \"mergeSchema\".\n",
    "            IGNORE_NEW_COLUMNS: drop DataFrame columns that do not exist in the table's schema.\n",
    "                Does nothing if the table does not yet exist in the Hive metastore.\n",
    "            OVERWRITE_SCHEMA: overwrite the table's schema with the DataFrame's schema.\n",
    "                This is the same as using the option \"overwriteSchema\".\n",
    "            RESCUE_NEW_COLUMNS: Create a new struct-type column to collect data for new columns.\n",
    "                This is the same strategy used in AutoLoader's rescue mode.\n",
    "                For more information: https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-evolution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        df_partitions = df.select(partition_columns).distinct()\n",
    "\n",
    "        if df_partitions.count() != 1:\n",
    "            raise ValueError(\"Found more than one partition value in the given DataFrame\")\n",
    "\n",
    "        # Build replaceWhere clause\n",
    "        replace_where_clauses = []\n",
    "        for partition_column, value in df_partitions.first().asDict().items():\n",
    "            replace_where_clauses.append(f\"`{partition_column}` = '{value}'\")\n",
    "        replace_where_clause = \" AND \".join(replace_where_clauses)\n",
    "\n",
    "        df_writer = (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", replace_where_clause)\n",
    "        )\n",
    "\n",
    "        # Set partition options\n",
    "        if len(partition_columns) > 0:\n",
    "            df_writer = df_writer.partitionBy(partition_columns)\n",
    "\n",
    "        # Set schema_evolution_mode options\n",
    "        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n",
    "            pass\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n",
    "            df_writer = df_writer.option(\"mergeSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n",
    "            if DeltaTable.isDeltaTable(spark, location):\n",
    "                table_columns = DeltaTable.forPath(spark, location).columns\n",
    "                new_df_columns = [col for col in df.columns if col not in table_columns]\n",
    "                df = df.drop(*new_df_columns)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n",
    "            df_writer = df_writer.option(\"overwriteSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Write to the delta table\n",
    "        df_writer.save(location)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _write_table_using_append_all(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        partition_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Write the DataFrame using APPEND_ALL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to modify.\n",
    "        location : str\n",
    "            Absolute Delta Lake path for the physical location of this delta table.\n",
    "        partition_columns : List[str], default=[]\n",
    "            The names of the columns used to partition the table.\n",
    "        schema_evolution_mode : Framework.SchemaEvolutionMode, default=ADD_NEW_COLUMNS\n",
    "            Specifies the way in which schema mismatches should be handled.\n",
    "            FAIL_ON_SCHEMA_MISMATCH: fail if the table's schema is not compatible with the DataFrame's.\n",
    "                This is the default Spark behavior when no option is given.\n",
    "            ADD_NEW_COLUMNS: schema evolution through adding new columns to the target table.\n",
    "                This is the same as using the option \"mergeSchema\".\n",
    "            IGNORE_NEW_COLUMNS: drop DataFrame columns that do not exist in the table's schema.\n",
    "                Does nothing if the table does not yet exist in the Hive metastore.\n",
    "            OVERWRITE_SCHEMA: overwrite the table's schema with the DataFrame's schema.\n",
    "                This is the same as using the option \"overwriteSchema\".\n",
    "            RESCUE_NEW_COLUMNS: Create a new struct-type column to collect data for new columns.\n",
    "                This is the same strategy used in AutoLoader's rescue mode.\n",
    "                For more information: https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-evolution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        df_writer = (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "        )\n",
    "\n",
    "        # Set partition options\n",
    "        if len(partition_columns) > 0:\n",
    "            df_writer = df_writer.partitionBy(partition_columns)\n",
    "\n",
    "        # Set schema_evolution_mode options\n",
    "        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n",
    "            pass\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n",
    "            df_writer = df_writer.option(\"mergeSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n",
    "            if DeltaTable.isDeltaTable(spark, location):\n",
    "                table_columns = DeltaTable.forPath(spark, location).columns\n",
    "                new_df_columns = [col for col in df.columns if col not in table_columns]\n",
    "                df = df.drop(*new_df_columns)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n",
    "            df_writer = df_writer.option(\"overwriteSchema\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Write to the delta table\n",
    "        df_writer.save(location)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _write_table_using_append_new(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        key_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Write the DataFrame using APPEND_NEW.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to modify.\n",
    "        location : str\n",
    "            Absolute Delta Lake path for the physical location of this delta table.\n",
    "        key_columns : List[str], default=[]\n",
    "            The names of the columns used to uniquely identify each record the table.\n",
    "            Used for APPEND_NEW, UPSERT, and TYPE_2_SCD load types.\n",
    "        schema_evolution_mode : Framework.SchemaEvolutionMode, default=ADD_NEW_COLUMNS\n",
    "            Specifies the way in which schema mismatches should be handled.\n",
    "            FAIL_ON_SCHEMA_MISMATCH: fail if the table's schema is not compatible with the DataFrame's.\n",
    "                This is the default Spark behavior when no option is given.\n",
    "            ADD_NEW_COLUMNS: schema evolution through adding new columns to the target table.\n",
    "                This is the same as using the option \"mergeSchema\".\n",
    "            IGNORE_NEW_COLUMNS: drop DataFrame columns that do not exist in the table's schema.\n",
    "                Does nothing if the table does not yet exist in the Hive metastore.\n",
    "            OVERWRITE_SCHEMA: overwrite the table's schema with the DataFrame's schema.\n",
    "                This is the same as using the option \"overwriteSchema\".\n",
    "            RESCUE_NEW_COLUMNS: Create a new struct-type column to collect data for new columns.\n",
    "                This is the same strategy used in AutoLoader's rescue mode.\n",
    "                For more information: https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-evolution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        # Set schema_evolution_mode options\n",
    "        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n",
    "            pass\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n",
    "            original_auto_merge = spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\")\n",
    "            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n",
    "            if DeltaTable.isDeltaTable(spark, location):\n",
    "                table_columns = DeltaTable.forPath(spark, location).columns\n",
    "                new_df_columns = [col for col in df.columns if col not in table_columns]\n",
    "                df = df.drop(*new_df_columns)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n",
    "            raise ValueError(\"OVERWRITE_SCHEMA is not supported in APPEND_NEW load type\")\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Build merge condition\n",
    "        merge_condition_parts = [f\"source.`{col}` = target.`{col}`\" for col in key_columns]\n",
    "        merge_condition = \" AND \".join(merge_condition_parts)\n",
    "\n",
    "        # Write to the delta table\n",
    "        delta_table = DeltaTable.forPath(spark, location)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(df.alias(\"source\"), merge_condition)\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "        # Reset spark.conf\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", original_auto_merge)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _write_table_using_upsert(\n",
    "        cls,\n",
    "        df: DataFrame,\n",
    "        location: str,\n",
    "        key_columns: List[str] = [],\n",
    "        schema_evolution_mode: SchemaEvolutionMode = SchemaEvolutionMode.ADD_NEW_COLUMNS,\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Write the DataFrame using UPSERT.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to modify.\n",
    "        location : str\n",
    "            Absolute Delta Lake path for the physical location of this delta table.\n",
    "        key_columns : List[str], default=[]\n",
    "            The names of the columns used to uniquely identify each record the table.\n",
    "            Used for APPEND_NEW, UPSERT, and TYPE_2_SCD load types.\n",
    "        schema_evolution_mode : Framework.SchemaEvolutionMode, default=ADD_NEW_COLUMNS\n",
    "            Specifies the way in which schema mismatches should be handled.\n",
    "            FAIL_ON_SCHEMA_MISMATCH: fail if the table's schema is not compatible with the DataFrame's.\n",
    "                This is the default Spark behavior when no option is given.\n",
    "            ADD_NEW_COLUMNS: schema evolution through adding new columns to the target table.\n",
    "                This is the same as using the option \"mergeSchema\".\n",
    "            IGNORE_NEW_COLUMNS: drop DataFrame columns that do not exist in the table's schema.\n",
    "                Does nothing if the table does not yet exist in the Hive metastore.\n",
    "            OVERWRITE_SCHEMA: overwrite the table's schema with the DataFrame's schema.\n",
    "                This is the same as using the option \"overwriteSchema\".\n",
    "            RESCUE_NEW_COLUMNS: Create a new struct-type column to collect data for new columns.\n",
    "                This is the same strategy used in AutoLoader's rescue mode.\n",
    "                For more information: https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-evolution\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        # Set schema_evolution_mode options\n",
    "        if schema_evolution_mode == cls.SchemaEvolutionMode.FAIL_ON_SCHEMA_MISMATCH:\n",
    "            pass\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.ADD_NEW_COLUMNS:\n",
    "            original_auto_merge = spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\")\n",
    "            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", True)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.IGNORE_NEW_COLUMNS:\n",
    "            if DeltaTable.isDeltaTable(spark, location):\n",
    "                table_columns = DeltaTable.forPath(spark, location).columns\n",
    "                new_df_columns = [col for col in df.columns if col not in table_columns]\n",
    "                df = df.drop(*new_df_columns)\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.OVERWRITE_SCHEMA:\n",
    "            raise ValueError(\"OVERWRITE_SCHEMA is not supported in UPSERT load type\")\n",
    "        elif schema_evolution_mode == cls.SchemaEvolutionMode.RESCUE_NEW_COLUMNS:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Build merge condition\n",
    "        merge_condition_parts = [f\"source.`{col}` = target.`{col}`\" for col in key_columns]\n",
    "        merge_condition = \" AND \".join(merge_condition_parts)\n",
    "\n",
    "        # Write to the delta table\n",
    "        delta_table = DeltaTable.forPath(spark, location)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(df.alias(\"source\"), merge_condition)\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "        # Reset spark.conf\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", original_auto_merge)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def csv_writer_named_file(  \n",
    "        cls,\n",
    "        path:str, \n",
    "        file_name:str, \n",
    "        df):\n",
    "        \"\"\"Write operation with rename csv file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Path to save the file.\n",
    "        file_name : str\n",
    "            File name.\n",
    "        df : DataFrame\n",
    "            PySpark DataFrame to save.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(f\"{path}/{file_name}\")\n",
    "            csv_file = [x.path for x in  dbutils.fs.ls(f'{path}/{file_name}') if x.path.endswith(\".csv\")][0]\n",
    "            dbutils.fs.mv(csv_file, f'{path}/{file_name}.csv')\n",
    "            dbutils.fs.rm(f'{path}/{file_name}', recurse = True)    \n",
    "            num_records_read = df.count()\n",
    "            num_records_write = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"{path}/{file_name}.csv\").count()\n",
    "            return cls._build_return_object(\n",
    "                status=cls.RunStatus.SUCCEEDED,\n",
    "                target_object=f\"{path}/{file_name}.csv\",\n",
    "                num_records_read=num_records_read,\n",
    "                num_records_loaded=num_records_write,\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            return cls._build_return_object(\n",
    "                status=cls.RunStatus.FAILED,\n",
    "                target_object=f\"{path}.{file_name}\",\n",
    "                num_records_read=num_records_read,\n",
    "                num_records_loaded=num_records_write,\n",
    "                error_message=str(e),\n",
    "                error_details=traceback.format_exc(),\n",
    "            )\n",
    "        \n",
    "    @classmethod\n",
    "    def _build_return_object(\n",
    "        cls,\n",
    "        status: RunStatus,\n",
    "        target_object: str,\n",
    "        num_records_read: int = 0,\n",
    "        num_records_loaded: int = 0,\n",
    "        error_message: str = \"\",\n",
    "        error_details: str = \"\",\n",
    "    ) -> ReturnObject:\n",
    "        \"\"\"Build the return object for a write operation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        status : Framework.RunStatus\n",
    "            Resulting status for this write operation.\n",
    "        target_object : str\n",
    "            Target object that we intended to write to.\n",
    "        num_records_read : int, default=0\n",
    "            Number of records read from the DataFrame.\n",
    "        num_records_loaded : int, default=0\n",
    "            Number of records written to the target table.\n",
    "        error_message : str, default=\"\"\n",
    "            Error message describing whichever error that occurred.\n",
    "        error_details : str, default=\"\"\n",
    "            Detailed error message or stack trace for the above error.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ReturnObject\n",
    "            Object containing the results of a write operation.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"target_object\": target_object,\n",
    "            \"num_records_read\": num_records_read,\n",
    "            \"num_records_loaded\": num_records_loaded,\n",
    "            \"num_records_errored_out\": num_records_read - num_records_loaded,\n",
    "            \"error_message\": error_message[:8000],\n",
    "            \"error_details\": error_details,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3ec5414-536a-4fd9-a218-a7e4bd5973f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">defined object Framework\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">defined object Framework\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "// TODO: can we recreate this in Python?\n",
    "object Framework {\n",
    "\n",
    "  import org.apache.spark.sql.DataFrame\n",
    "  import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "  import org.apache.spark.sql.execution.datasources.LogicalRelation\n",
    "\n",
    "  def dumpDataFrameDependencies(df: DataFrame): Unit = {\n",
    "    val catalogTablesFromLogicalPlan = new PartialFunction[LogicalPlan, String] {\n",
    "      def apply(logical: LogicalPlan) = {\n",
    "        val catalogTable = logical.asInstanceOf[LogicalRelation].catalogTable.get\n",
    "        val database = catalogTable.identifier.database\n",
    "        val table = catalogTable.identifier.table\n",
    "        if (database.isDefined) s\"${database.get}.${table}\" else table\n",
    "      }\n",
    "\n",
    "      def isDefinedAt(logical: LogicalPlan) = {\n",
    "        (logical.isInstanceOf[LogicalRelation]\n",
    "          && logical.asInstanceOf[LogicalRelation].catalogTable.isDefined)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    val dependencies = (\n",
    "      df.queryExecution.analyzed\n",
    "      .collectWithSubqueries(catalogTablesFromLogicalPlan)\n",
    "      .distinct\n",
    "    )\n",
    "\n",
    "    println(\"Dumping dependencies:\")\n",
    "    dependencies.foreach {\n",
    "      println\n",
    "    }\n",
    "  }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "notebookOrigID": 4200710190909963,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
